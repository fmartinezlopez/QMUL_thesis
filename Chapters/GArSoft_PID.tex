\chapter{Particle identification in ND-GAr}
\label{chapter:garsoft_pid}

\begin{chapquote}{Charlotte BrontÃ«, \textit{Jane Eyre}}
	I am no bird; and no net ensnares me; I am a free human being with an independent will.
\end{chapquote}

% more on ND-GAr and the ND, and why we want to do this!
In \gls{dune} Phase II, \gls{ndgar} will fulfill the role of TMS measuring the momentum and sign of the charged particles exiting \gls{ndlar}. Additionally, it will measure neutrino interactions inside the \gls{hpgtpc}. This way, \gls{ndgar} will constrain certain cross-section systematic uncertainties and study the effect of \gls{fsi} in \gls{cc} interactions. To do so, it needs to measure the spectrum of protons and charged pions at low energies, as well as the pion multiplicity. This puts strong requirements on the particle identification (\gls{pid}) capabilities of the detector, as well as stimulating the relevant developments in the reconstruction.

The goal of the present Chapter is to provide an overview of the current status and design of the GArSoft package, the simulation and reconstruction software of \gls{ndgar}, and present the contributions and upgrades that I have implemented to enhance the reconstruction with the \gls{pid} in mind. These contributions include:
\begin{itemize}
	\item developing the calibration of the \gls{hpgtpc} to enable \gls{pid} at low momenta using calorimetry,
	\item designing a strategy to use the information from the \gls{ecal} and \gls{muid} for separating muons and charged pions,
	\item and using a combination of the \gls{hpgtpc} and \gls{ecal} to perform a time-of-flight measurement for identifying protons at high momenta.
\end{itemize}

\section{GArSoft}\label{section:garsoft}

GArSoft is a software package developed for the simulation and reconstruction of events in \gls{ndgar}. It is inspired by the LArSoft toolkit \cite{Church2013} used for the simulation of \gls{lartpc} experiments, like the \gls{dune} \gls{fd} modules. It is based on \texttt{art}, the framework for event processing in particle physics experiments \cite{ART}. Its main dependencies are \texttt{ROOT} \cite{Brun1997}, \texttt{GENIE} \cite{Andreopoulos2009,Andreopoulos2015} and \texttt{Geant4} \cite{GEANT42002,GEANT42006,GEANT42016}. It allows the user to run all the steps of a generation-simulation-reconstruction workflow using Fermilab Hierarchical Configuration Language (FHiCL) files.

\subsection{Event generation}

The standard generator FHiCLs in GArSoft run the event generation and particle propagation simulation (i.e. \texttt{Geant4}) in the same job by default. However, it is possible to split them up if needed. The current version of GArSoft provides five different event generators, each of them producing \texttt{simb::MCTruth} products. The available modules are:
\begin{itemize}
	\item \texttt{SingleGen}: particle gun generator. It produces the specified particles with a given distribution of momenta, initial positions and angles.
	\item \texttt{TextGen}: text file generator. The input file must follow the \texttt{hepevt} format, the module simply copies the event records into \texttt{simb::MCTruth} objects.
	\item \texttt{GENIEGen}: \texttt{GENIE} neutrino event generator. The module runs the neutrino-nucleus interaction generator using the options specified in the driver FHiCL file (flux file, flavour composition, number of interactions per event, $t_{0}$ distribution, ...). Current default version is \texttt{v3_04_00}, tune \texttt{G18}.
	\item \texttt{RadioGen}: radiological generator. It produces a set list of particles to model radiological decays.
	\item \texttt{CRYGen}: cosmic ray generator. The module runs the \texttt{CRY} event generator \cite{Hagmann2007} with a configuration specified in the FHiCL file (for example latitude and altitude of the detector, and energy threshold).
\end{itemize}

\begin{comment}
	\footnote{In brief, each event contains at least two  lines.  The first line contains two entries, the event number and the number of particles in the event. Each following line contains 15 entries to describe each particle. The entries are: status code, pdg code for the particle, entry of the first mother for this particle, entry of the second mother for this particle, entry of the first daughter for this particle, entry of the second daughter for this particle, $x$ component of the particle momentum, $y$ component of the particle momentum, $z$ component of the particle momentum, energy of the particle, mass of the particle, $x$ component of the particle initial position, $y$ component of the particle initial position, $z$ component of the particle initial position, and time of the particle production.}
\end{comment}

The module \texttt{GArG4} searches for all the generated \texttt{simb::MCTruth} data products, using them as inputs to the \texttt{Geant4} simulation with the specified detector geometry. The current version of the simulated \gls{ndgar} geometry is that described in section \ref{subsec:ndgar_design}. A constant $0.5~\mathrm{T}$ magnetic field along the drift coordinate is assumed. The main outputs of this step are \texttt{simb::MCParticle} objects for the generated \texttt{Geant4} particles, \texttt{gar::EnergyDeposit} data products for the energy deposits in the \gls{hpgtpc} and \texttt{gar::CaloDeposit} data products for the energy deposits in the \gls{ecal} and muon system.

\subsection{Detector simulation}

The standard detector simulation step in GArSoft is all run with a single FHiCL, but the different modules can be run independently as well. First the \texttt{IonizationReadout} module simulates the charge readout of the \gls{hpgtpc}, and later the \texttt{SiPMReadout} module runs twice, once for the \gls{ecal} and then for the muon system, with different configurations.

The \texttt{IonizationAndScintillation} module collects all the \texttt{gar::EnergyDeposit} data products, to compute the equivalent number of ionisation electrons for each energy deposit. The \texttt{ElectronDriftAlg} module simulates the electron diffusion numerically, both in the longitudinal and transverse directions, and applies an electron lifetime correction factor. The induced charge on the nearest and neighbouring readout pads is modeled using the provided pad response functions. The digitisation of the data is then simulated with the \texttt{TPCReadoutSimAlg} module. By default, the \gls{adc} sampling rate used is $50.505~\mathrm{MHz}$. The resulting raw waveforms for each channel are stored with zero-suppression, in order to save memory and CPU time. The algorithms keep blocks of \gls{adc} values above a certain threshold, plus some adjustable additional early and late tick counts. The results of these three steps are \texttt{gar::raw::RawDigit} data products.

For the \gls{ecal} and the muon system the \texttt{SiPMReadout} module calls either the \texttt{ECALReadoutSimStandardAlg} or \texttt{MuIDReadoutSimStandardAlg} modules. These take all the \texttt{gar::CaloDeposit} data products in the corresponding detector, and perform the digitisation depending on whether the hit was in a tile or strip layer. They include single photon statistics, electronic noise, SiPM saturation and time smearing. The resulting objects are \texttt{gar::raw::CaloRawDigit} data products.

\subsection{Reconstruction}

The reconstruction in GArSoft is also run as a single job by default. It first runs the hit finding, clustering, track fitting and vertex identification in the \gls{hpgtpc}, followed by the hit finding and clustering in the \gls{ecal} and muon system. After those, it produces the associations between the tracks and the \gls{ecal} clusters.

Focusing first on the \gls{hpgtpc} reconstruction, the \texttt{CompressedHitFinder} module takes the zero-suppressed \gls{adc}s from the \texttt{gar::raw::RawDigit} data products. The reconstructed hits largely correspond to the above-threshold blocks, however the hit finder identifies waveforms with more than one maximum, diving them into multiple hits if they dip below a certain threshold. The data products produced are of the form \texttt{gar::rec::Hit}. These are the inputs to the clustering of hits in the \texttt{TPCHitCluster} module. Hits close in space and time are merged, and the resulting centroids are found. This module outputs \texttt{gar::rec::TPCClusters} objects and associations to the input hits.

The following step prior to the track fitting is the pattern recognition. The module called \texttt{tpcvechitfinder2} uses the \texttt{gar::rec::TPCClusters} data products to find track segments, typically called vector hits. They are identified by performing linear 2D fits to the positions of the clusters in a $10~\mathrm{cm}$ radius, one fit for each coordinate pair. For each direction, the sum of slopes (in absolute value) in the 2D fits is computed. Setting the independent variable as the direction with the smallest slope sum, a final 3D fit defines the line segment of the vector hit. The clusters are merged into a given vector hit if they are less than $2~\mathrm{cm}$ away from the line segment. The outputs are \texttt{gar::rec::VecHit} data products, as well as associations to the clusters. The \texttt{tpcpatrec2} module takes the \texttt{gar::rec::VecHit} objects to form the track candidates. The vector hits are merged together if their direction matches, their centres are within $60~\mathrm{cm}$ and their direction vectors point roughly to their respective centres. Once the clusters of vector hits are formed, they are used to make a first estimation of the track parameters simply taking three clusters along the track. The module produces \texttt{gar::rec::Track} data products and associations between these tracks and the clusters and vector hits.

The track is fitted by means of a Kalman filter in the \texttt{tpctrackfit2} module, using the position along the drift direction as the independent variable. Two different fits are performed per track, a forward and a backwards fit, each starting from one of the track ends. The Kalman filter state vector $(y,z,R,\phi,\mathrm{tan}\lambda)$ is estimated at each point along the track using a Bayesian update. The track parameters reported in the forward and backwards fits are the ones computed at the opposite end where the fit started. The main outputs of the track fit are the \texttt{gar::rec::Track} objects. Additionally, the module stores the fitted 3D positions along the track in the \texttt{gar::rec::TrackTrajectory} data products, and the total charge and step sizes for each point also get stored in the form of \texttt{gar::rec::TrackIonization} objects.

\begin{figure}[t]
	\centering
	\includegraphics[width=.90\linewidth]{Images/GArSoft_PID/gar_workflow.pdf}
	\caption{Schematic diagram showing the different modules involved in the \gls{ndgar} neutrino event production.}
	\label{fig:gar_workflow}
\end{figure}

After the tracking step, the \texttt{vertexfinder1} module looks at the reconstructed \texttt{gar::rec::Track} products, creating vertex candidates with the track ends that are within $12~\mathrm{cm}$ of each other. The vertices are then fitted using linear extrapolations from the different track ends associated. The results are \texttt{gar::rec::Vertex} data products, and associations to the tracks and corresponding track ends.

For the \gls{ecal} and muon tagger, the \texttt{SiPMHitFinder} module runs twice with different configurations, adapted to the particular capabilities of both. The module simply takes the \texttt{gar::raw::CaloRawDigit} products, applies a calibration factor to convert the \gls{adc} counts to $\mathrm{MeV}$, and for the strip layer hits it calculates the position along the strip using the times recorded by both SiPMs. This module produces \texttt{gar::rec::CaloHit} data products. Next, these objects are used as inputs to the \texttt{CaloClustering} module. It merges the hits based on a simple nearest neighbours (NN) algorithm. For the resulting clusters it also computes the total energy and position of the centroid. The results are stored as \texttt{gar::rec::Cluster} data products, with associations to the hits.

The last step in the reconstruction is associating the reconstructed tracks in the \gls{hpgtpc} to the clusters formed in the \gls{ecal} and \gls{muid}. The \texttt{TPCECALAssociation} module checks first the position of the track end points, considering only the points that are at least $215~\mathrm{cm}$ away from the cathode or have a radial distance to the centre greater than $230~\mathrm{cm}$. The candidates are propagated up to the radial position (in the case of clusters in the barrel) or the drift coordinate position (for the end cap clusters) of the different clusters in the collection using the track parameters computed at the end point. The end point is associated to the cluster if certain proximity criteria are met. This module creates associations between the tracks, the end points and the clusters. The criteria for the associations are slightly different for the \gls{ecal} and the \gls{muid}.

Figure \ref{fig:gar_workflow} shows the simulation and reconstruction workflow of neutrino event production in \gls{ndgar}. The diagram also presents other external packages used to produce analysis files.

\section[\texorpdfstring{$\mathrm{d}E/\mathrm{d}x$}{dE/dx} measurement in the TPC]{\boldmath\texorpdfstring{$\mathrm{d}E/\mathrm{d}x$}{dE/dx} measurement in the TPC}\label{section:dEdx}

Among the parameters extracted from the track fitting, ionisation is particularly useful for particle identification, as it is a function of the particle velocity. For the case of relativistic particles this dependence is not very strong, but measuring the track on a large number of points may allow us to estimate the amount of ionisation accurately. This, paired with a measurement of the momentum, may allow us to identify the particle type.

The first calculation of the energy loss per unit length for relativistic particles using a quantum-mechanical treatment is due to H. Bethe \cite{Bethe1930}. Using this approach, the mean ionisation rate of a charged particle traveling through a material medium is (using natural units $G=\hbar=c=1$):
\begin{equation}\label{Eq:3.1}
    \expval{\frac{\mathrm{d}E}{\mathrm{d}x}} = \frac{4 \pi N e^{4}}{m_{e}\beta^{2}} z^{2} \left(\mathrm{log} \frac{2m_{e}\beta^{2}\gamma^{2}}{I} - \beta^{2}\right),
\end{equation}
where $N$ is the number density of electrons in the medium, $e$ the elementary charge, $m_{e}$ is the electron mass, $z$ the charge of the particle in units of $e$, $\beta$ is the velocity of the particle, $\gamma = (1-\beta^{2})^{-1}$, and $I$ denotes the effective ionisation potential averaged over all electrons. This relation is known as the Bethe-Bloch formula.

From Eq. (\ref{Eq:3.1}) one can see that the ionisation loss does not depend explicitly on the mass of the charged particle, that for non-relativistic velocities it falls as $\beta^{-2}$, then goes through a minimum, and increases as the logarithm of $\gamma$. This behaviour at high velocities is commonly known as the relativistic rise. The physical origin of this effect is partly due to the fact that the transverse electromagnetic field of the particle is proportional to $\gamma$, therefore as it increases so does the cross section.

It was later understood that the relativistic rise could not grow indefinitely with $\gamma$. A way to add this feature in the Bethe-Bloch formula is by introducing the so-called density effect term. It accounts for the polarisation effect of the atoms in the medium, which effectively shield the electromagnetic field of the charged particle halting any further increase of the energy loss \cite{Fermi1940}. Denoting the correction as $\delta(\beta)$, one can rewrite Eq. (\ref{Eq:3.1}) as:
\begin{equation}\label{Eq:3.2}
    \expval{\frac{\mathrm{d}E}{\mathrm{d}x}} = \frac{4 \pi N e^{4}}{m_{e}\beta^{2}} z^{2} \left(\mathrm{log} \frac{2m_{e}\beta^{2}\gamma^{2}}{I} - \beta^{2}-\frac{\delta(\beta)}{2}\right).
\end{equation}

In general, the form of $\delta(\beta)$ depends on the medium and its state of aggregation, involving the usage of empirical parametrisations and tabulated values \cite{Sternheimer1984}.

Another standard method to compute the amount of ionisation a charged particle produces is the so-called photo-absorption ionisation (PAI) model proposed by W. Allison and J. Cobb \cite{Allison1980}. Within their approach, the mean ionisation is evaluated using a semiclassical calculation in which one characterises the continuum material medium by means of a complex dielectric constant $\varepsilon(k, \omega)$, which is a function of the energy and momentum exchanged between the moving particle and the atoms and electrons in the medium, $\omega$ and $k$. However, in order to model the dielectric constant they rely on the quantum-mechanical picture of photon absorption and collision. Therefore, in the PAI model the computation of the ionisation loss involves a numerical integration of the measured photo-absorption cross section for the relevant material.

In a particle physics experiment, the typical way of determining the energy loss per unit length as a function of the particle velocity is studying identified particles over a range of momenta. Once we have established this relation we can use it for other, unknown particles. In this sense, it makes sense to have a regular mathematical expression for this relation that one can use.

It happens that neither the Bethe-Bloch theory nor the PAI model from Allison and Cobb offer a closed mathematical form for the ionisation curve. This is the reason why a full parametrisation of the ionisation curves can be useful. A parametrisation originally proposed for the \gls{aleph} \gls{tpc} \cite{Blum2008} and later used by the \gls{alice} \gls{tpc} \cite{ALICETPC2013} group that manages to capture the features of the ionisation energy loss is:
\begin{equation}\label{Eq:3.3}
    f(\beta\gamma) = \frac{P_{1}}{\beta^{P_{4}}}\left(P_{2}-\beta^{P_{4}}-\mathrm{log}\left[P_{3}+\frac{1}{(\beta\gamma)^{P_{5}}}\right]\right),
\end{equation}
where $P_{i}$ are five free parameters. Hereafter, I will refer to Eq. (\ref{Eq:3.3}) as the \gls{aleph} $\mathrm{d}E/\mathrm{d}x$ parametrisation.

\subsection[Truncated \texorpdfstring{$\mathrm{d}E/\mathrm{d}x$}{dE/dx} mean]{Truncated \boldmath\texorpdfstring{$\mathrm{d}E/\mathrm{d}x$}{dE/dx} mean} \label{subsec:mean_dEdx}

GArSoft provides a collection of charge deposits for each reconstructed track. In order to obtain the mapping between charge and energy in the \gls{hpgtpc}, I produced an \gls{mc} sample consisting of single, isotropic protons. The starting points of the protons were sampled inside a $50\times50\times25 \ \mathrm{cm}$ box centred at $(100, -150, 1250)~\mathrm{cm}$, and their momenta are uniformly distributed in the range $0.25 - 1.75 ~ \mathrm{GeV}$. The details of the energy calibration method developed are given in App. \ref{section:dEdx_calibration}.

Once we have a collection of $\mathrm{d}E/\mathrm{d}x$ values for each track, we can compute the corresponding most probable ionisation loss per unit length of the particle. This is the value predicted by the Bethe-Bloch or the PAI models, and together with a measurement of the momentum it allows for particle identification.

However, estimating the most probable $\mathrm{d}E/\mathrm{d}x$ value for each reconstructed track is not a trivial task, as the $\mathrm{d}E/\mathrm{d}x$ follows a Landau-like distribution \cite{Landau1944}. Therefore, one should perform e.g. a LanGauss\footnote{I use the term LanGauss to refer to a Landau distribution convolved with a Gaussian. In the literature, this distribution is often referred to as Landau+Gaussian or \texttt{langau}.} fit to correctly estimate the most probable values. Automating these kinds of fits is often problematic, as they usually incur in non-convergence problems. Moreover, the reconstructed $\mathrm{d}E/\mathrm{d}x$ distributions we obtain tend to have relatively low statistics, which may also produce poor fits. In practice, doing these unsupervised fits may degrade our performance, and a more robust method is preferred.

A possibility could be taking the mean of the reconstructed $\mathrm{d}E/\mathrm{d}x$ distribution for each particle. The problem with this approach is that the high energy Landau tail, combined with our limited statistics, can induce large fluctuations in the computation of the mean. Imagine you have two protons with the same kinetic energy, but due to reconstruction problems (or mere chance) in one case you do not get as many charge deposits reconstructed in its high ionisation loss region. If you do not remove the tails, the computed $\mathrm{d}E/\mathrm{d}x$ means will be significantly different.

\begin{figure}[t]
	\centering
	\includegraphics[width=.70\linewidth]{Images/GArSoft_PID/dEdx/reco_dEdx_truncation_comp_alt.pdf}
	\caption[Fractional residuals between the true and the reconstructed calibrated $\mathrm{d}E/\mathrm{d}x$ means and the $60\%$ truncated means.]{Fractional residuals between the true and the reconstructed calibrated $\mathrm{d}E/\mathrm{d}x$ means (blue) and the $60\%$ truncated means (yellow), for each event in the stopping proton sample.}
	\label{fig:energy_trucation_comp}
\end{figure}

In order to avoid those fluctuations, one can compute the mean of a truncated $\mathrm{d}E/\mathrm{d}x$ distribution instead. By keeping only a given fraction of the lowest energy deposits we obtain an estimate of the mean energy loss that is more resilient to reconstruction inefficiencies and statistical effects. Figure \ref{fig:energy_trucation_comp} shows a comparison between the $\left<\mathrm{d}E/\mathrm{d}x\right>$ computed by taking the mean of the full distribution (blue line) and the $60\%$ lowest energy clusters (yellow line), for the stopping proton sample. The fractional residuals are computed for each proton, taking the corresponding means using their collections of true and reconstructed energy deposits. One can see that using the simple mean translates into a high bias and uncertainty in the $\left<\mathrm{d}E/\mathrm{d}x\right>$ estimation, whereas applying the truncation reduces both significantly.

\begin{comment}
	Additionally, I performed a comparison between the $60\%$ truncated mean $\mathrm{d}E/\mathrm{d}x$ obtained using the different methods discussed earlier, namely the calibrated but uncorrected (blue), the calibrated and corrected (yellow) and the uncalibrated but corrected (red) distributions. The results are shown in Fig. \ref{fig:energy_trucation_comp} (right panel). While the widths of these distributions are similar, the bias obtained for the corrected sample, i.e. calibration function and correction factor applied, is a factor of $\sim2$ lower than in the uncalibrated case and almost three times smaller than for the uncorrected sample.
\end{comment}

\begin{figure}[t]
	\centering
	\includegraphics[width=.90\linewidth]{Images/GArSoft_PID/dEdx/reco_dEdx_truncation_opt.pdf}
	\caption[Estimated values of the mean $\mathrm{d}E/\mathrm{d}x$ bias and resolution for the stopping proton sample at different values of the truncation factor.]{Estimated values of the mean $\mathrm{d}E/\mathrm{d}x$ bias (left panel) and resolution (right panel) obtained using the calibrated data from the stopping proton sample, for different values of the truncation factor.}
	\label{fig:energy_trucation_opt}
\end{figure}

The next step is to optimise the level of truncation we are going to apply to our data. To do so, I use different truncation factors, i.e. the percentage of energy-ordered reconstructed energy deposits we keep to compute the mean calibrated $\mathrm{d}E/\mathrm{d}x$ values of the stopping proton sample. Then, following the same procedure of computing the fractional residuals as before, I fit the resulting histograms using a double Gaussian function. This is simply the sum of two Gaussian functions of the type:
\begin{equation}
	g(x;~\mu, \sigma, A) = A~\mathrm{e}^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}.
\end{equation}
I do not add the classical normalisation factor of the Gaussian, $1/\sqrt{2\pi}\sigma$, therefore the amplitude $A$ simply represents the maximum of the function. One of the two Gaussian functions describes the core part of the distribution, while the other captures the behaviour of the tails.

For each truncation factor, I look at the bias and the resolution I obtain. I define these as the weighted means of the corresponding parameters in the fits:
\begin{equation}\label{eq:weighted_par_double_gauss}
	\bar{x} = \frac{A_{core}~x_{core}+A_{tail}~x_{tail}}{A_{core}+A_{tail}},
\end{equation}
where $A_{core}$ and $A_{tail}$ are the amplitudes of the core and tail distributions, respectively, and $x$ is either the mean $\mu$ or the width $\sigma$ of said distributions.

Figure \ref{fig:energy_trucation_opt} shows the bias (left panel) and the resolution (right panel) I obtain for the stopping proton sample, using different values of the truncation. From these, it can be seen that a truncation factor of $50\%$ minimises the bias in the estimation, while $70\%$ gives the best resolution. That way, I settled on the intermediate value of $60\%$ truncation, which yields a $\left<\mathrm{d}E/\mathrm{d}x\right>$ resolution of $5.00\pm0.08~\%$ for stopping protons.

\subsection[Mean \texorpdfstring{$\mathrm{d}E/\mathrm{d}x$}{dE/dx} parametrisation]{Mean \boldmath\texorpdfstring{$\mathrm{d}E/\mathrm{d}x$}{dE/dx} parametrisation} \label{subsec:dEdx_parametrisation}

\begin{figure}[t]
	\centering
	\includegraphics[width=.85\linewidth]{Images/GArSoft_PID/dEdx/dEdx_betagamma_examples.pdf}
	\caption[Examples of the truncated mean $\mathrm{d}E/\mathrm{d}x$ LanGauss fits for various $\beta\gamma$ bins, from a simulated \gls{fhc} neutrino sample.]{Examples of the truncated mean $\mathrm{d}E/\mathrm{d}x$ LanGauss fits for various $\beta\gamma$ bins, from a simulated \gls{fhc} neutrino sample.}
	\label{fig:dEdx_betagamma_fits}
\end{figure}

Now that we have a way to estimate the mean energy loss of a particle in the \gls{hpgtpc}, we can determine the value of the free parameters in the \gls{aleph} formula, Eq. (\ref{Eq:3.3}). For this, I produced a sample of $10^{5}$ reconstructed \gls{fhc} neutrino events inside \gls{ndgar}. In this case I do not use the stopping proton sample, as we need to cover the full kinematic range of interest for the neutrino interactions in our detector.

Among the reconstructed data objects, the sample does not include an estimation of the velocity of the tracks. Instead, the tracks have a value for the reconstructed momentum and the associated \gls{pdg} code of the \texttt{Geant4}-level particle that created the track. Therefore, one can extract some of the tracks in the sample. In this case I select the ones associated to electrons, muons, pions and protons, and compute $\beta$ and $\gamma$ using the reconstructed momentum and their mass. In terms of $\beta\gamma$ the mean $\mathrm{d}E/\mathrm{d}x$ does not depend on the particle species, so one can consider the dataset as a whole. For this fit, I will express $\beta$ in terms of the $\beta\gamma$ product as:
\begin{equation}
    \beta = \frac{\beta\gamma}{\sqrt{1+(\beta\gamma)^{2}}},
\end{equation}
which can be easily proven from the definition of $\gamma$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.85\linewidth]{Images/GArSoft_PID/dEdx/mcmc_weighted_equal_frequency_bin_200.pdf}
	\caption[Resulting one and two dimensional projections of the posterior probability distributions of the \gls{aleph} $\left<\mathrm{d}E/\mathrm{d}x\right>$ parameters.]{Resulting one and two dimensional projections of the posterior probability distributions of the \gls{aleph} $\left<\mathrm{d}E/\mathrm{d}x\right>$ parameters obtained by fitting the $60\%$ truncated mean $\mathrm{d}E/\mathrm{d}x$ values from a \gls{fhc} neutrino sample in \gls{ndgar}. The vertical dashed lines in the 1D distributions represent the 16th, 50th and 84th percentiles.}
	\label{fig:dEdx_aleph_fit}
\end{figure}

Next, I bin the sample in $\beta\gamma$. I chose a fine binning so as to capture the different features of the ionisation curve. Instead of fixing the bin width, I select them so each one has approximately the same statistics. Then, for each $\beta\gamma$ slice, I compute the median and the interquartile range (IQR) of the $\left<\mathrm{d}E/\mathrm{d}x\right>$ distribution. Using these, I make a histogram in the range $[\mathrm{median}-\mathrm{IQR},~ \mathrm{median}+5~\mathrm{IQR})$, which I fit to a LanGauss function in order to extract the MPV. Using this range accounts for the asymmetric nature of the distributions, while also helps avoiding a second maximum present at low $\beta\gamma$, probably as a result of reconstruction failures.

\begin{figure}[t]
	\centering
	\includegraphics[width=.85\linewidth]{Images/GArSoft_PID/dEdx/dEdx_aleph_data_with_fit.pdf}
	\caption[Truncated mean $\mathrm{d}E/\mathrm{d}x$ as a function of the $\beta\gamma$ product, together with the best fit obtained using the \gls{aleph} parametrisation.]{Truncated mean $\mathrm{d}E/\mathrm{d}x$ obtained for the \gls{fhc} neutrino sample as a function of the $\beta\gamma$ product (upper panel). Also shown are the fitted most probable values for each $\beta\gamma$ bin (red points) and the best fit obtained using the \gls{aleph} parametrisation (black line). The residuals resulting from the fit are shown in the lower panel.}
	\label{fig:dEdx_betagamma_aleph}
\end{figure}

A few examples of these fits are shown in Fig. \ref{fig:dEdx_betagamma_fits}. The chosen values of $\beta\gamma$ sit at very distinct points along the $\left<\mathrm{d}E/\mathrm{d}x\right>$ curve, going from the high ionisation region at low velocities (top left panel), to the minimum point (top right panel), the beginning of the relativistic rise (bottom left panel), and the plateau produced by the density effect (bottom right panel).

I use the resulting most probable $\left<\mathrm{d}E/\mathrm{d}x\right>$ values and the centres of the $\beta\gamma$ bins as the points to fit to the \gls{aleph} formula. For this particular fit, I employ the least-squares method to get a first estimation of the \gls{aleph} parameters. Applying uniform priors, I then use these values as the starting point of a $10^{5}$ step Markov chain \gls{mc}. Figure \ref{fig:dEdx_aleph_fit} shows the posterior probability distributions I obtain for each parameter. The reported best fit points are based on the 16th, 50th, and 84th percentiles in the marginalised distributions.

\begin{figure}[t]
	\centering
	\includegraphics[width=.90\linewidth]{Images/GArSoft_PID/dEdx/dEdx_curves_with_fit.pdf}
	\caption[Distribution of the $60\%$ truncated mean $\mathrm{d}E/\mathrm{d}x$ versus reconstructed momentum for the \gls{fhc} neutrino sample.]{Distribution of the $60\%$ truncated mean $\mathrm{d}E/\mathrm{d}x$ versus reconstructed momentum for the \gls{fhc} neutrino sample. The black lines indicate the predictions of the \gls{aleph} parametrisation for electrons, muons, charged pions and protons.}
	\label{fig:dEdx_vs_momentum}
\end{figure}

The resulting fit (black line), compared to the data points (red points) and the underlying distribution is shown in Fig. \ref{fig:dEdx_betagamma_aleph} (top panel). The overall fit is good, with a reduced chi-squared of $\chi^{2}/ndf=1.02$. However, there are some regions where the fit does not describe the data correctly, like the very low $\beta\gamma$ regime, where the fit severely underestimates the $\mathrm{d}E/\mathrm{d}x$ for energy losses $\geq 50 ~ \mathrm{keV}/\mathrm{cm}$, and the start of the relativistic rise, where we have a slight overestimation. This is a result of those points having a larger uncertainty when compared to the ones around the dip or the plateau areas. These differences can be better seen in the residual plot, Fig. \ref{fig:dEdx_betagamma_aleph} (bottom panel).

\begin{comment}
\begin{table}[t]
	\caption{Best fit parameters obtained fitting a \gls{ndgar} simulated \gls{fhc} neutrino sample to the \gls{aleph} mean $\mathrm{d}E/\mathrm{d}x$ parametrisation from Eq. (\ref{Eq:3.3}).}
	\begin{center}
		\begin{small}
			\begin{tabular}{l|l|l}
				Parameter & Best fit $\pm 1\sigma$ & $3\sigma$ range \\ \hline
				$P_{1}$   &                        &                 \\
				$P_{2}$   &                        &                 \\
				$P_{3}$   &                        &                 \\
				$P_{4}$   &                        &                 \\
				$P_{5}$   &                        &                
			\end{tabular}
		\end{small}
	\end{center}
	\label{tab:aleph_fit}
\end{table}
\end{comment}

It is interesting to look at the results of the fit in momentum space, for the different particle species. Figure \ref{fig:dEdx_vs_momentum} shows the truncated mean $\mathrm{d}E/\mathrm{d}x$ values versus the reconstructed momentum for the neutrino sample. Using a logarithmic scale for the momentum helps with visualising the curves corresponding to the various particles. The resulting fits for electrons, muons, pions and protons are also shown (solid black lines). Notice that each curve stops at a different momentum value, as the fits only extend up to $\beta\gamma = 200$ and translating this limit into momentum depends on the particle.

From Fig. \ref{fig:dEdx_vs_momentum}, the particle separation power of the $\left<\mathrm{d}E/\mathrm{d}x\right>$ measurement is evident. In the low momentum regime, below $300~\mathrm{MeV}/c$, separating muons and pions should be possible using this method, even if it is with a low significance. On the other hand, protons can be reliably identified up to $1.5~\mathrm{GeV}/c$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.90\linewidth]{Images/GArSoft_PID/dEdx/proton_dEdx_momentum.pdf}
	\caption[Estimated values of the mean $\mathrm{d}E/\mathrm{d}x$ bias and resolution obtained for the true protons in a \gls{fhc} neutrino sample as a function of the momentum.]{Estimated values of the mean $\mathrm{d}E/\mathrm{d}x$ bias (left panel) and resolution (right panel) obtained for the true protons in the \gls{fhc} neutrino sample as a function of the reconstructed momentum.}
	\label{fig:proton_dEdx_momentum}
\end{figure}

Relevant to the separating power is the $\left<\mathrm{d}E/\mathrm{d}x\right>$ resolution. This can be obtained from the fit, by taking the ratio of the difference between the expected energy loss for a given particle type and momentum and the measured value over the expectation. Then, performing a double Gaussian fit we can extract the bias and the resolution by means of Eq. (\ref{eq:weighted_par_double_gauss}). Figure \ref{fig:proton_dEdx_momentum} presents the values of the $\left<\mathrm{d}E/\mathrm{d}x\right>$ bias (left panel) and resolution (right panel) as a function of the momentum for the true protons in the neutrino sample.

When compared to the values for the resolution obtained for the stopping proton sample (see e.g. Fig. \ref{fig:energy_trucation_opt}), it appears that the resolution is worse. For that low energy sample the resolution obtained was $5\%$, whereas now we only achieve those numbers for momenta $\geq 0.75~\mathrm{GeV}/c$. However, there are several differences between these two cases. The former was obtained for a single proton sample, with tracks fully contained in the detector volume. On top of that, I refined the selection requiring a single reconstructed track per event, which eliminates any misreconstruction effects. In this case, we are dealing with tracks that may have fragmented, or even have contributions from different true particles. Also, note that at low energies the $\left<\mathrm{d}E/\mathrm{d}x\right>$ for protons is much higher than it is for other particles. Therefore, having a poor resolution in that range does not have an impact on the proton separation.

\section{Muon and pion separation in the ECal and MuID}\label{section:muon_bdt}

\begin{figure}[t]
	\centering
	\includegraphics[width=.70\linewidth]{Images/GArSoft_PID/BDT/ndgar_fhc_numu_cc_mu_spectrum.pdf}
	\caption[True momentum distributions for the primary muon and the post \gls{fsi} charged pions in $\nu_{\mu}$ \gls{cc} $N\pi^{\pm}$ interactions inside \gls{ndgar}.]{True momentum distribution for the primary muon in $\nu_{\mu}$ \gls{cc} $N\pi^{\pm}$ interactions inside the fiducial volume of \gls{ndgar} (blue line), compared to the post \gls{fsi} charged pion spectrum (red line).}
	\label{fig:primary_muon_spectrum}
\end{figure}

As it can be seen from Fig. \ref{fig:dEdx_vs_momentum}, it is not possible to separate muons and charged pions in the \gls{hpgtpc} using $\mathrm{d}E/\mathrm{d}x$ for momenta $\geq 300~\mathrm{MeV}/c$. In \gls{ndgar}, approximately $70\%$ of the interactions in \gls{fhc} mode will be $\nu_{\mu}$ \gls{cc} (compared to the $47\%$ of $\bar{\nu}_{\mu}$ \gls{cc} interactions when operating in \gls{rhc} mode), while $24\%$ are neutral currents. Out of these, around $53\%$ and $47\%$ of them will produce at least one charged pion in the final state, respectively. Figure \ref{fig:primary_muon_spectrum} shows a comparison between the spectra of the primary muons and the charged pions for $\nu_{\mu}$ \gls{cc} interactions in \gls{ndgar} producing one or more charged pions. From this, one can see that (i) the majority of muons and charged pions are not going to be distinguishable with a $\left<\mathrm{d}E/\mathrm{d}x\right>$ measurement, and that (ii) particle identification is necessary both to classify correctly the $\nu_{\mu}$ \gls{cc} events and identify the primary muon within them.

\gls{ndgar} features two other subdetectors which can provide additional information for this task, namely the \gls{ecal} and \gls{muid}. The current \gls{ecal} design, described in section \ref{subsec:ndgar_design}, consists of $42$ layers, made of $5~\mathrm{mm}$ of Pb, $7~\mathrm{mm}$ of plastic scintillator, and an additional $1~\mathrm{mm}$ PCB board for the tile layers. The total thickness of this calorimeter is $1.66$ nuclear interaction lengths, or $1.39$ pion interaction lengths. The \gls{muid} design is in a more conceptual stage, however it is envisioned to feature layers with $10~\mathrm{cm}$ of Fe and $2~\mathrm{cm}$ of plastic scintillator. With its three layers, it will have a thickness of $1.87$ or $1.53$ nuclear or pion interaction lengths, respectively.

\begin{comment}
	\footnote{It is not mentioned anywhere, but I assume that there should also be another layer of PCB board of $1~\mathrm{mm}$. However, in this case its contribution to the total thickness of the sampling calorimeter would be negligible.}
\end{comment}

Because pion showers are dominated by inelastic nuclear interactions, the signatures of these particles in the calorimeter will look significantly different from those of muons, or in general any minimum ionising particle (MIP). Although our \gls{ecal} is not thick enough to fully contain the hadronic showers of the charged pions at their typical energies in \gls{fhc} neutrino interactions, they can still be used to understand whether the original particle was more hadron-like or MIP-like. In Fig. \ref{fig:ecal_example} I show two examples of energy distributions created by a muon (left panel) and a charged pion (right panel) of similar momenta interacting in the \gls{ecal}. These figures represent the transverse development of the interactions. For each of them, I computed the principal component and centre of mass of the interaction, projecting the position of the hits onto the plane perpendicular to that direction, and taking the distances relative to the centre. It can be seen that the muon follows an almost MIP-like behaviour, with most of the deposited energy located in the central bin. On the other hand, the pion not only deposits more energy overall, but also this energy is more spread-out among the different hits. It is this kind of information that would allow us to distinguish muons from pions.

This way, I identify three main action points that need to be addressed if one wants to use these detectors to distinguish between muons and charged pions. These are:
\begin{enumerate}
	\item the way we make the associations between tracks in the \gls{hpgtpc} to the activities (what in GArSoft we call clusters) in the \gls{ecal} and the \gls{muid},
	\item what variables or features one can extract from the calorimeters that encapsulate the information we are interested about,
	\item and how to carry out the classification problem.
\end{enumerate}

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/ecal_energy_distribution_example.pdf}
	\caption[Distributions of energy deposits in the \gls{ecal} for a muon and a charged pion with similar momenta.]{Distributions of energy deposits in the \gls{ecal} for a muon (left panel) and a charged pion (right panel) with similar momenta. The energy is projected onto the plane perpendicular to the principal component of the hits, and the positions are relative to the centre of the interaction.}
	\label{fig:ecal_example}
\end{figure}

\subsection{Track-ECal matching}

One of the main factors in the muon and pion separation is the way we associate clusters in the \gls{ecal} to reconstructed tracks in the \gls{hpgtpc}. Missing some associations or making wrong ones can bias the \gls{ecal} quantities that we can use for classifying particles. The current algorithm in GArSoft provides precise associations, i.e. most of the associations that it produces are correct, but it appears to miss an important number of them.

The current \gls{tpc} track-\gls{ecal} cluster association algorithm is divided in four parts. It first checks whether the track end point fulfils certain conditions to be extrapolated. There are two cut values in this step, one for the drift direction and other for the radial.

If the point can be extrapolated, the code computes the coordinates of the centre of curvature using the Kalman fit estimates at the track end $(y, \ z, \ 1/R, \ \phi, \ \mathrm{tan}\lambda)$. It then compares the distance between this and the cluster in the $(z,y)$ plane to $R$. This introduces another cut in the perpendicular direction.

The next step is different for clusters in the barrel or in one of the end caps. If it is a barrel cluster the algorithm extrapolates the track up to the radial distance of the cluster. There are three possible outcomes, the extrapolated helix can cut the cylinder of radius $r_{cluster}$ two, one or zero times. It gets the cut point that is closer to the cluster and checks that it is either in the barrel or the end caps. Computing the difference between the $x$ coordinates of the cluster and the extrapolated point, the module checks that this is not greater than a certain value. If the cluster is in an end cap, it propagates the track up to the $x$ position of the cluster. Then, the algorithm computes the angle in the $(z,y)$ plane between the centre of curvature and the cluster, $\alpha$, and the centre of curvature and the propagated point, $\alpha'$. A cut is applied to the quantity $(\alpha-\alpha')R$.

If the cluster contains more than a certain number $N$ of hits, it applies an extra cut to the dot product of the direction of the track at the propagated point and the cluster direction.

The code makes sure to only associate one end of the track (if any) to a cluster. However, it can associate more than one track to the same cluster. This makes sense, as different particles can contribute to the same cluster in the \gls{ecal}, but it makes it difficult to quantify the relative contributions of the tracks to a certain cluster.

As a way of gauging the performance of this algorithm, I developed a new, simpler association module. The goal was to have a simple and robust algorithm, which depends on as few parameters as possible and that can produce a one-to-one matching between tracks and \gls{ecal} clusters.

For each reconstructed track, the new algorithm applies the same procedure to the forward and the backward fits irrespective of their end point positions. It first gets the Kalman fit parameters at the corresponding end point, together with the position along the drift direction, $x_{0}$ and $(y_{0}, \ z_{0}, \ 1/R, \ \phi_{0}, \ \mathrm{tan}\lambda)$.

For each \gls{ecal} cluster, I compute the radial distance to the centre of the \gls{tpc} and find the $\phi$ value in the range $[\phi_{0}, \ \phi_{0}+\mathrm{sign}(R)\phi_{max})$ that makes the propagated helix intersect with the circle defined with such radius. The $(x,y,z)$ position of the helix for the $\phi$ value found (if any) is then computed. In case there are two intersections, I keep the one that minimises the distance between $(y, z)$ and $(y_{cluster}, z_{cluster})$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.70\linewidth]{Images/GArSoft_PID/associations/helix_propagation_metrics_no_t0.pdf}
	\caption[Performance of the new track-cluster association algorithm for different values of the $\chi^{2}$ cut.]{Comparison between the precision (blue), sensitivity (yellow) and $F_{1}$ score (red) obtained for the default (horizontal lines) and new algorithms, both with the $\chi^{2}$-based direction estimator (squares) and cheated directions (circles), for different values of the $\chi^{2}$ cut.}
	\label{fig:associations_no_t0}
\end{figure}

\begin{comment}
Figure \ref{fig:associations_no_t0} (left panel) shows an example track (red line) being propagated up to $\phi_{0}+\mathrm{sign}(R)\pi/2$ (dashed blue line). The image also shows the \gls{ecal} clusters present in the event (green squares). For each of them, the algorithm will try to find the intersections of the propagated helix and the circles defined with their corresponding radii.
\end{comment}

I then calculate a $\chi^{2}$ value based on the Euclidean distance between the propagated point and the cluster:
\begin{equation}
	\chi^{2}/ndf = \frac{\sum_{n=0}^{2}\left(x^{(n)}-x^{(n)}_{c}\right)^{2}}{3}.
\end{equation}
If there was no intersection I set the stored value to $-1$. In the end, for each reconstructed track in the event one ends up with two collections of $\chi^{2}$ values, one for each \gls{ecal} cluster and fit directions.

The current code only supports having \gls{ecal} clusters associated to one end of each track. We have two options to decide what track end to keep. The first one tries to cheat the selection, looking at the distance between the two track ends and the true start position of the associated \gls{mc} particle. The second one keeps the track end with more $\chi^{2}$ entries below a certain cut value.

This feature of only considering one track end limits the algorithm, making it unsuitable for reconstructing events with particles originating outside the \gls{hpgtpc}. However, as for the moment our main concern is the study of neutrino interactions in the gaseous argon volume, this is an acceptable assumption.

In order to associate a cluster to a track, I take all clusters with a $\chi^{2}$ value in the range $[0, \chi^{2}_{cut})$. If a cluster has been assigned to more than one track we leave it with the one with the lowest $\chi^{2}$.

This default behaviour of the algorithm can be modified to associate more than one track to each cluster. Not only that, but the $\chi^{2}$ values can be used to assign relative weights to the different contributions.

To evaluate the performance of the association method, I use a binary classification approach. In this case, I check the leading \gls{mc} Track IDs associated to the reconstructed tracks and \gls{ecal} clusters. I count an association as true positive (TP) if both Track IDs coincide. An association is considered false positive (FP) when the Track IDs are different. If a cluster has not been associated to any track but it shares the Track ID with a reconstructed track it is counted as a false negative (FN).

For the testing, I produced a sample of $10^{4}$ \gls{fhc} neutrino events inside the \gls{hpgtpc}. Figure \ref{fig:associations_no_t0} shows the precision (blue line), sensitivity (yellow line), and $F_{1}$ score (red line) I obtain for different values of $\chi^{2}_{cut}$. For comparison, the same metrics computed for the default algorithm with the current configuration are also shown (dashed lines). In the case of the new algorithm, I use both the $\chi^{2}$-based method to estimate the track direction described earlier (square markers) and the cheated direction from the \texttt{Geant4}-level information (circle markers). For either of these we achieve similar values of the precision compared to the old code, while having a considerably higher sensitivity. It can be seen that cheating the direction of the tracks only makes a difference at high $\chi^{2}_{cut}$, past the optimal value of the cut around the $F_{1}$ score maximum. Therefore, I set the $\chi^{2}$ method as the default.

\begin{figure}[t]
	\centering
	\includegraphics[width=.55\linewidth]{Images/GArSoft_PID/associations/drift_correction.png}
	\caption[Schematics of a method to deal with track-\gls{ecal} associations in non-zero $t_{0}$ neutrino interaction events using the cluster time.]{Schematics of a possible option to deal with track-\gls{ecal} associations in non-zero $t_{0}$ neutrino interaction events, trying to correct for the drift direction uncertainty in a cluster-by-cluster basis using the cluster time, $t_{cluster}$.}
	\label{fig:associations_drift}
\end{figure}

One of the possible weak points of this approach is that it relies on the position along the drift direction to make the decisions. Within the current \gls{ndgar} design implemented in GArSoft, the timing information is provided by the \gls{ecal}. That effectively means that prior to making the track-\gls{ecal} associations the reconstructed $x$ positions of the track trajectories differ from the simulated ones by an amount:
\begin{equation}
	x_{reco}^{(n)} - x_{sim}^{(n)} = v_{drift} \ t_{0},
\end{equation}
where $v_{drift}$ is the mean drift velocity in our medium and the initial time is in the range $t_{0}\in[0, t_{spill})$, with $t_{spill}$ being the spill length. For a $10 \ \mu\mathrm{s}$ spill this translates into a maximum $30 \ \mathrm{cm}$ uncertainty on the drift direction position.

The current default in GArSoft sets $t_{0} = 0$, but the functionality to randomly sample this within the spill time is in place. Therefore, we need to understand what is the impact of a non-zero $t_{0}$ on the associations algorithm and foresee possible ways of minimising a loss in performance.

\begin{figure}[t]
	\centering
	\includegraphics[width=.70\linewidth]{Images/GArSoft_PID/associations/helix_propagation_metrics_ecal.pdf}
	\caption[Performance of the new track-cluster association algorithm when applying the cluster $t_{0}$ correction for different values of the $\chi^{2}$ cut.]{Comparison between the precision (blue), sensitivity (yellow) and $F_{1}$ score (red) obtained for the new algorithm when applying the cluster $t_{0}$ correction (squares) and when no correction is applied (circles), for different values of the $\chi^{2}$ cut.}
	\label{fig:associations_t0_correction}
\end{figure}

Figure \ref{fig:associations_drift} represents a possible option to tackle the association problem when having events with a non-zero initial time $t_{0}$. The black and white circles represent the original points, whereas the squares indicate the corrected positions. The end points of the track and the propagated points up to the cluster radius are indicated using filled and unfilled markers, respectively. The red square represents the position of the cluster.

Here I try to correct for the drift coordinate position using the time associated to the cluster. Assuming that the drift time is much larger than the propagation time, $t_{cluster}$ could be used as a good estimation of the $t_{0}$. An alternative can be using the earliest time associated to a hit in said cluster. Doing this for each cluster before computing the $\chi^{2}$ value could be used as an alternative to knowing the specific value of the $t_{0}$, as when the association is correct this will provide the right correction but its impact is small enough to not change the position significantly in the case the cluster does not correspond to a given track.

I tested the effect of this correction again using a sample of $10^{4}$ \gls{fhc} neutrino events, this time with $t_{0}$ values uniformly distributed within the spill. Figure \ref{fig:associations_t0_correction} shows the performance of the algorithm for the case where the cluster $t_{0}$ correction is applied (square markers), and for the no correction case (circle markers), as a function of $\chi^{2}_{cut}$. In this case, the differences are particularly noticeable at low values of the cut. This makes sense, as the $t_{0}$ effect becomes subdominant when the distance we consider grows large. Overall, the correction increases the sensitivity while keeping the precision almost unchanged. As a result, I apply the $t_{0}$ correction to the generated samples by default.

\subsection{Classification strategy}

The problem of the muon and charged pion separation has to be viewed in the broader context of the particle identification in our detector. Focusing on the beam neutrino interactions, it is clear that we are going to have muons and pions spanning a broad momentum range. Not only that, but we will also have other particles with similar characteristics that will make the classification even more challenging. Therefore, we are presented with a task that will depend heavily on the kinematic range we are looking at each time, as both the available information and the possible impurities of other particle species vary.

\begin{figure}[t]
	\centering
	\includegraphics[width=.80\linewidth]{Images/GArSoft_PID/BDT/fraction_vs_preco_no_duplicates.pdf}
	\caption[Momentum distributions for the reconstructed muons and charged pions in a \gls{fhc} sample, together with the fraction of them reaching the \gls{ecal} and \gls{muid}.]{Momentum distributions for the reconstructed muons (top panel) and charged pions (bottom panel) in a \gls{fhc} neutrino sample, together with the fraction of them reaching the \gls{ecal} (red) and \gls{muid} (blue). Each entry corresponds to a reconstructed track, backtracked to a true muon or pion which has not produced any other reconstructed track.}
	\label{fig:fraction_particles_ecal_muid}
\end{figure}

For instance, distinguishing muons from pions could be difficult at low momenta, as a great number of them do not reach the \gls{ecal}. Therefore, we could think of tailoring a version of the classification for that particular case, which could be complemented with a $\mathrm{d}E/\mathrm{d}x$ measurement. Likewise, for momenta $\geq 1~\mathrm{GeV}$ muons and pions reach the calorimeters efficiently, but so do protons. Because of this, one can try to train another classifier for this energy range, and rely on other methods to remove as many of the protons as possible.

Figure \ref{fig:fraction_particles_ecal_muid} shows the momentum distribution of the reconstructed muons (top) and pions (bottom) in a \gls{fhc} sample. It also contains the fraction of particles reaching the \gls{ecal} (red) and \gls{muid} (blue), for the different momentum bins. In Fig. \ref{fig:dEdx_vs_momentum_regions} I show the mean $\mathrm{d}E/\mathrm{d}x$ of different particles as a function of the momentum, computed using the \gls{aleph} parametrisation with the best fit parameters found in section \ref{subsec:mean_dEdx}.

\begin{figure}[t]
	\centering
	\includegraphics[width=.80\linewidth]{Images/GArSoft_PID/BDT/dEdx_fit_only.pdf}
	\caption[Predicted truncated $\expval{\mathrm{d}E/\mathrm{d}x}$ versus momentum, for electrons, muons, charged pions and protons, obtained using the \gls{aleph} parametrisation]{Predicted truncated mean $\mathrm{d}E/\mathrm{d}x$ versus momentum, for electrons, muons, charged pions and protons, obtained using the \gls{aleph} parametrisation. The vertical dashed lines represent the boundaries of the six regions used for the muon and pion classification training.}
	\label{fig:dEdx_vs_momentum_regions}
\end{figure}

Using these two figures as references, I decided to approach the classification by dividing the problem into six different momentum regions. A summary of these can be found in Tab. \ref{tab:bdt_regions}. The idea is to select a number of regions where the basic classification can be complemented with other methods. For the problem at hand, I prepared separate samples of isotropic single muons and pions, with momenta uniformly distributed along the corresponding momentum range. Each sample contains $5 \times 10^{4}$ events of the corresponding particle species. I do not generate samples for the first region, as it is assumed that the separation can be achieved using $\mathrm{d}E/\mathrm{d}x$ only. For the last region, I generate particles up to a momentum of $10~\mathrm{GeV}/c$, as that is well above the typical energies of muons and pions from \gls{fhc} neutrino interactions in \gls{ndgar}. In all cases, the ratios I keep between the training and testing datasets are $80:20$.

Additionally, I prepared another sample of $10^{5}$ \gls{fhc} neutrino events. For each interaction, I select the reconstructed particles which were backtracked to true muons or charged pions. I use this dataset to perform validation checks, to see how the models trained with the single particle data generalise to a more realistic scenario.

\begin{table}[t]
	\caption{Momentum ranges and description of the \gls{pid} approach assumed for the muon and pion classification task.}
	\begin{center}
		\begin{small}
			\begin{tabular}{p{2.7cm}|l}
				Momentum range                                    & Description                                                                          \\[1mm] \hline \rule{0pt}{1.1\normalbaselineskip}
				$< 0.1$ \hfill $\mathrm{GeV}/c$      & All tracks can be separated with $\mathrm{d}E/\mathrm{d}x$                           \\[2mm]
				$[0.1,~0.3)$ \hfill $\mathrm{GeV}/c$ & Use \gls{ecal} for reaching muons and pions, $\mathrm{d}E/\mathrm{d}x$ for the rest        \\[2mm]
				$[0.3,~0.8)$ \hfill $\mathrm{GeV}/c$ & Use \gls{ecal} for muons and pions, $\mathrm{d}E/\mathrm{d}x$ for protons                  \\[2mm]
				$[0.8,~1.5)$ \hfill $\mathrm{GeV}/c$ & Use \gls{ecal} and \gls{muid} for muons and pions,  $\mathrm{d}E/\mathrm{d}x$ for protons        \\[2mm]
				$[1.5,~3.0)$ \hfill $\mathrm{GeV}/c$ & Use \gls{ecal} and \gls{muid} for muons and pions, \gls{tof} for protons                               \\[2mm]
				$\geq 3.0$ \hfill $\mathrm{GeV}/c$   & Use \gls{ecal} and \gls{muid} for muons and pions, $\mathrm{d}E/\mathrm{d}x$ and \gls{tof} for protons
			\end{tabular}
		\end{small}
	\end{center}
	\label{tab:bdt_regions}
\end{table}

To tackle this classification problem, I make use of Boosted Decision Trees (\gls{bdt}). A decision tree uses a flowchart-like structure to make decisions based on some input data. It starts from a root node, which represents the complete dataset, and then it splits this based on the variable or feature which gives the best separation between classes, creating two new nodes. The process repeats for each node until it reaches a certain limit, like a maximum number of splits or some tolerance criteria. The last set of nodes are often called leaf nodes, and represent the final prediction of the classifier. A good example of the use of \gls{bdt}s in particle physics is that from Ref. \cite{Cornell2021}.

Boosting refers to a family of methods to combine the predictions from multiple classifiers, following a sequential approach where each new model learns from the errors of the previous one. The process starts with a simple decision tree, which is used to make predictions on the training data. Then, the data points misclassified by the first model are assigned higher weights, and another decision tree is trained on the data with adjusted weights. The predictions of the two trees are then combined, and the cycle repeats for a predefined number of iterations. Gradient boosting uses the direction of the steepest error descent to guide the learning process and improve the accuracy with each iteration.

\subsection{Feature selection and importance}

Using the reconstructed tracks as a starting point, I compute a number of \gls{ecal} and \gls{muid} variables for each of them. As there can be more than one cluster associated to a track, I collect all associated clusters and compute these variables from the complete collection of associated hits. For the \gls{muid}, because it only features three layers and typically there will be fewer hits, I also allow single hits to be associated with tracks\footnote{At the reconstruction level non-clustered hits are put into single hit clusters, instead of being thrown away. This is necessary to keep the consistency of the track-cluster association code.}. I can roughly divide the variables in three types: energy-related, geometry-related and statistical. In the following, I briefly describe the variables related exclusively to the \gls{ecal}:

\begin{itemize}
	\item \textbf{Energy-related ECal}
	\begin{itemize}
		\item \gls{ecal} total energy (ClusterTotalEnergy): sum of the energy of all the \gls{ecal} hits.
		\item Mean \gls{ecal} hit energy (HitMeanEnergy): mean of the hit energy distribution.
		\item Standard deviation \gls{ecal} hit energy (HitStdEnergy): standard deviation of the hit energy distribution.
		\item Maximum \gls{ecal} hit energy (HitMaxEnergy): maximum of the hit energy distribution.
	\end{itemize}
	\item \textbf{Geometry-related ECal}
	\begin{itemize}
		\item Mean distance hit-to-cluster (DistHitClusterMean): mean of the distance distribution between the hits and the corresponding cluster's main axis.
		\item RMS distance hit-to-cluster (DistHitClusterRMS): root mean square of the distance distribution between the hits and the corresponding cluster's main axis.
		\item Maximum distance hit-to-centre (DistHitCenterMax): maximum of the distance distribution between the hits and the centre of the \gls{hpgtpc}.
		\item Time-of-Flight velocity (TOFVelocity): slope obtained when fitting a straight line to the hit time versus hit distance to the centre (i.e. $d = v \times t$).
	\end{itemize}
	\item \textbf{Energy and geometry ECal}
	\begin{itemize}
		\item Radius 90\% energy (Radius90E): distance in the hit-to-cluster distribution for which 90\% of the total energy is contained in the hits that are closer to the axis (i.e. radius that contains 90\% of the energy).
	\end{itemize}
	\item \textbf{Statistical ECal}
	\begin{itemize}
		\item Number of hits (NHits): total number of hits associated to the track.
		\item Number of layers with hits (NLayers): difference between the last and the first layer with hits.
	\end{itemize}
\end{itemize}

\begin{figure}[p!]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/ecal_feature_distribution_all_example.pdf}
	\caption[Example \gls{ecal} feature distributions for muons and charged pions in the five different momentum ranges considered.]{Example \gls{ecal} feature distributions for muons (blue) and charged pions (red) in the five different momentum ranges considered (from top to bottom, in ascending momentum order). From left to right: mean hit energy, mean distance hit-to-cluster, and number of layers with hits.}
	\label{fig:ecal_feature_example}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/muid_feature_distribution_all_example.pdf}
	\caption[Example \gls{muid} feature distributions for muons and charged pions in the three different momentum ranges considered.]{Example \gls{muid} feature distributions for muons (blue) and charged pions (red) in the three different momentum ranges considered (from top to bottom, in ascending momentum order). From left to right: standard deviation hit energy, maximum distance hit-to-hit, and number of hits.}
	\label{fig:muid_feature_example}
\end{figure}

Figure \ref{fig:ecal_feature_example} shows the distributions of three different \gls{ecal} variables, separating true muons (blue) and charged pions (red), for the five momentum ranges considered. I chose to show one feature from each category, namely the mean energy per hit (left column), the mean distance between the hits and the centre of the cluster (middle column), and the number of \gls{ecal} layers with hits (right column). These give an idea of the separating power of the different features, and how it changes considerably with the energy. In the number of layers with hits distributions, the peak at $6$ appears because the first six \gls{ecal} layers sit inside the pressure vessel. Therefore, some of the particles get stopped crossing it, never making it to the seventh layer.

In the case of the \gls{muid}, because at low momenta a significant fraction of the particles do not make it past the \gls{ecal}, I only consider the information coming from this detector for momenta $\geq 0.8~\mathrm{GeV}/c$, i.e. for the last three momentum regions. The variables I extract from it are the following:
\begin{itemize}
	\item \textbf{Energy-related MuID}
	\begin{itemize}
		\item \gls{muid} total energy (ClusterMuIDTotalEnergy): sum of the energy of all the \gls{muid} hits.
		\item Mean \gls{muid} hit energy (HitMuIDMeanEnergy): mean of the \gls{muid} hit energy distribution.
		\item Standard deviation \gls{muid} hit energy (HitMuIDStdEnergy): standard deviation of the \gls{muid} hit energy distribution.
		\item Maximum \gls{muid} hit energy (HitMuIDMaxEnergy): maximum of the \gls{muid} hit energy distribution.
	\end{itemize}
	\item \textbf{Geometry-related MuID}
	\begin{itemize}
		\item Maximum distance \gls{muid} hit-to-hit (DistHitMuIDMax): maximum distance between pairs of \gls{muid} hits.
		\item Maximum distance \gls{muid} hit-to-centre (DistHitCenterMuIDMax): maximum of the distance distribution between the \gls{muid} hits and the centre of the \gls{hpgtpc}.
	\end{itemize}
	\item \textbf{Statistical MuID}
	\begin{itemize}
		\item Number of hits (NHitsMuID): total number of \gls{muid} hits associated to the track.
		\item Number of layers with hits (NLayersMuID): difference between the last and the first layer with \gls{muid} hits.
	\end{itemize}
\end{itemize}

Figure \ref{fig:muid_feature_example} shows the distributions of three different \gls{muid} variables, separating true muons (blue) and charged pions (red), for the three momentum ranges which use the muon tagger information. In this case I decided to show the standard deviation of the \gls{muid} hit energy distribution (left column), the maximum distance between the \gls{muid} hit pairs (middle column), and the number of \gls{muid} hits (right column). These variables are used together with the \gls{ecal} features at high momenta, providing additional disambiguation power.

For both the \gls{ecal} and the \gls{muid}, I add as an extra feature the ratio between the total energy from the clusters and the momentum measured in the \gls{hpgtpc} (ECalRatioFWD and MuIDRatioFWD, as I use the momentum value from the forward fit). This variable is typically used for electron discrimination in calorimeters, as the small difference between their momentum and kinetic energy makes it peak around $1$. I also include another time-of-flight variable, measured with both the \gls{ecal} and \gls{muid} from the arrival times to each detector (TOFMuID).

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/summary_pca_p0_0.55_sigmap_0.25.pdf}
	\caption[Results of the \gls{pca}, Shapley and Gini feature importance analyses for the momentum range $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$.]{Left panel: cumulative explained variance for the first three principal components (top panel) and contribution of the different features to the principal axes in feature space (bottom panel). Right panel: Shapley (blue) and Gini (red) feature importances for the different input features. Both figures correspond to the samples in the momentum range $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$.}
	\label{fig:bdt_pca_importance_ii}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/summary_pca_p0_1.15_sigmap_0.35.pdf}
	\caption[Results of the \gls{pca}, Shapley and Gini feature importance analyses for the momentum range $0.8 \leq p < 1.5 ~ \mathrm{GeV}/c$.]{Left panel: cumulative explained variance for the first three principal components (top panel) and contribution of the different features to the principal axes in feature space (bottom panel). Right panel: Shapley (blue) and Gini (red) feature importances for the different input features. Both figures correspond to the samples in the momentum range $0.8 \leq p < 1.5 ~ \mathrm{GeV}/c$.}
	\label{fig:bdt_pca_importance_iii}
\end{figure}

\begin{comment}
\begin{figure}[t]
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/importance/feature_importance_p0_1.15_sigmap_0.35_MuID.pdf}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/importance/feature_importance_p0_2.25_sigmap_0.75_MuID.pdf}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/importance/feature_importance_p0_6.50_sigmap_3.50_MuID.pdf}
	\end{subfigure}
	\caption{Left panel: number of non-decaying, decaying and decaying in the fiducial volume pions for a \gls{mc} sample of $10^{5}$, $p=500 \ \mathrm{MeV}$ isotropic positively charged pions inside the \gls{tpc}. Right panel: event display for a positive pion decaying inside the fiducial volume, with a single reconstructed track for the pion and muon system.}
	\label{fig:pion_decays}
\end{figure}
\end{comment}

Once our features have been defined, one can do some exploratory analysis to understand how well the variables describe the target class, and avoid the black-box approach by checking which features are most relevant for the learning process. This way, I perform a feature analysis for each of the momentum ranges I divided this classification problem into. It follows three steps: first a principal component analysis (\gls{pca}), followed by a feature importance study using Gini \cite{Jost2006} and Shapley \cite{Shapley1951} values, and finally a feature permutation importance analysis.

The \gls{pca} is useful to understand the variance of the feature space. It is an unsupervised machine learning technique that allows the user to perform a dimensionality reduction \cite{Jolliffe2016}. It uses a singular value decomposition (SVD) of the input features to project them into a lower dimensional space. The idea is to find the matrix $\mathbf{C}_{q}$, whose columns are the first $q$ orthonormal eigenvectors of the input covariance matrix. Consider the $n \times p$ real matrix of input data $\mathbf{X}$, where $n$ is the number of samples and $p$ the number of features. If $\mathbf{X}$ is centred, i.e. the means of its columns are equal to zero, we can write the covariance matrix of $\mathbf{X}$ as $\mathbf{C}=\mathbf{X}^{\intercal}\mathbf{X}/(n-1)$. This matrix can be diagonalised, yielding:
\begin{equation}
	\mathbf{C}=\mathbf{V}\mathbf{L}\mathbf{V}^{\intercal},
\end{equation}
where $\mathbf{V}$ is a matrix of eigenvectors and $\mathbf{L}$ a diagonal matrix with eigenvalues $\lambda_{i}$. Then, performing SVD on $\mathbf{X}$ gives us:
\begin{equation}
	\mathbf{X}=\mathbf{U}\mathbf{S}\mathbf{W}^{\intercal},
\end{equation}
where $\mathbf{U}$ is a unitary matrix, whose columns are called left singular vectors, $\mathbf{S}$ is a diagonal matrix of singular values $s_{i}$, and $\mathbf{W}$ is another unitary matrix, its columns known as right singular vectors. This way, we can write:
\begin{equation}
	\mathbf{C}=\mathbf{W}\mathbf{S}\mathbf{U}^{\intercal}\mathbf{U}\mathbf{S}\mathbf{W}^{\intercal}/(n-1)=\mathbf{W}\frac{\mathbf{S}^{2}}{n-1}\mathbf{W}^{\intercal}.
\end{equation}
meaning that the right singular vectors are also the eigenvectors of the covariance matrix. The SVD can be computed numerically following an iterative approach \cite{Golub1970}.

This way, taking an input data vector $X \in \mathbb{R}^{p}$, the resulting feature vector $Y \in \mathbb{R}^{q}$ is given by:
\begin{equation}
	Y = \mathbf{C}_{q}^{\intercal} X.
\end{equation}
The new features capture most of the variance of the original sample, while being lower dimensional, as $q<p$.

Before applying the \gls{pca} reduction one needs to centre and scale the input data. Centring is necessary when using SVD to obtain the eigenvectors of the covariance matrix, as only in that case we can do the identification with the right singular vectors from the input data. Scaling is needed when variables are on different scales, as some can then dominate the \gls{pca} procedure.

I use the \gls{pca} module of \texttt{scikit-learn} \cite{scikit-learn}, together with the \texttt{RobustScaler}, which centres the data and scales it based on the interquartile range. In Fig. \ref{fig:bdt_pca_importance_ii} (left panel) and Fig. \ref{fig:bdt_pca_importance_iii} (left panel) I show the results I obtained from the \gls{pca} for the momentum ranges $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$ and $0.8 \leq p < 1.5 ~ \mathrm{GeV}/c$, respectively. Notice that in the second case the number of features increases considerably, as this is the first region which uses the \gls{muid} variables. I find that, in all the cases, adding a fourth PC does not add additional information. As it can be seen in the top panels of the figures, the cumulative explained variance is already over $80\%$ with three PCs.

The bottom panels show the contribution of the variables to the principal axes. For the two first momentum regions, I observe a tendency of the energy-related and the geometry-related \gls{ecal} variables to be clustered together. For the other ranges, when I include the \gls{muid} variables, there seems to be a division between \gls{ecal} and \gls{muid} variables. For these, it seems like the number of \gls{ecal} layers with hits also plays an important role.

The next step in the analysis is to quantify the importance of the features based on two additional metrics, namely the Gini and the Shapley values. The Gini importance, often called mean decrease impurity, is based on how much a feature contributes to the purity improvement at the splits in each decision tree. The purity is measured in terms of the Gini impurity index, defined as \cite{Jost2006}:
\begin{equation}
	I_{G} = 1 - \sum_{i} f_{i},
\end{equation}
where $f_{i}$ is the fractional abundance of the $i$-th class. Then, for each split one can compute the weighted decrease in impurity as:
\begin{equation}
	\Delta_{G} = \frac{N_{t}}{N} \left(I_{G} - \frac{N_{t}^{R}}{N_{t}} I_{G}^{R} - \frac{N_{t}^{L}}{N_{t}} I_{G}^{L}\right),
\end{equation}
where $N$ represents the total number of samples, $N_{t}$ the number of samples at the current node, $N_{t}^{R}$ and $N_{t}^{L}$ the number of samples in the right and left children respectively, $I_{G}$ is the Gini impurity at the current node, and $I_{G}^{R}$ and $I_{G}^{L}$ the Gini impurities of the resulting right and left children.

For each decision tree, one will have a normalised vector with the accumulated decrease in Gini impurity for each feature. In the case of a \gls{bdt}, the feature importances are simply the mean for all the estimators in the ensemble.

\begin{comment}
\footnote{Note to self: this appears not to be the case. If you get the \texttt{feature_importance} for each tree in the \gls{bdt} and take the average, the result is not the same to be one reported in the \texttt{feature_importance} attribute of the \gls{bdt}.}.
\end{comment}

\begin{figure}[t]
	\centering
	\includegraphics[width=.85\linewidth]{Images/GArSoft_PID/BDT/summary_top_six.pdf}
	\caption{Evolution of the SHAP importance for the top six most important features across all five momentum ranges.}
	\label{fig:bdt_shap_regions}
\end{figure}

The concept of Shapley values originated in the context of game theory, and it measures the marginal contribution of a feature in enhancing the accuracy of a classifier \cite{Shapley1951}. Take $F$ to be the set of all features in a problem, and $S \subseteq F$ a subset of features. To compute the Shapley value of the $i$-th feature, one has to train a model with that feature present, $f_{S \cup \{i\}}$, and another model trained without it, $f_{S}$. This has to be repeated for all possible combinations of subsets $S \subset F \setminus \{i\}$, and evaluating the models predictions on the appropriate sets of data $x_{S}$. This way, the Shapley value results \cite{SHAP2017}:
\begin{equation}
	\varphi_{i} = \sum_{S \subset F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} \left[f_{S \cup \{i\}}(x_{S \cup \{i\}}) - f_{S}(x_{S})\right].
\end{equation}

I train the \texttt{GradientBoostingClassifier} from \texttt{scikit-learn} with the default configuration in order to evaluate both the Gini and Shapley importances. The Gini scores are automatically computed by \texttt{scikit-learn}, using the training data. For the Shapley importance, I use the implementation from the \texttt{SHAP} package \cite{SHAP2017}, computing it using the test sample. The results can be seen in Fig. \ref{fig:bdt_pca_importance_ii} (right panel) and Fig. \ref{fig:bdt_pca_importance_iii} (right panel), again for the momentum ranges $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$ and $0.8 \leq p < 1.5 ~ \mathrm{GeV}/c$. The length of the bars denote either the SHAP (blue) or the Gini (red) importance of the feature. One interesting thing to notice is that, when looking at the Gini importance, there is always one feature that dominates over the rest. This is not the case for the SHAP importance, where importances tend to be more balanced.

\begin{figure}[p]
	\centering
	\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/summary_permutation.pdf}
	\caption[Permutation importances for the ten most important features in the different momentum ranges.]{Permutation importances for the ten most important features in the different momentum ranges (from left to right, top to bottom, in increasing momentum order). The bars indicate the effect that permutations of each feature have on the purity (blue) and the sensitivity (yellow), the translucent regions representing one standard deviation around the central value.}
	\label{fig:bdt_permutations}
\end{figure}

Across all momentum ranges, I observe that the most important features are rather consistent. For the five regions considered, only six variables sit in the top five at least once. Figure \ref{fig:bdt_shap_regions} shows the evolution of the SHAP importance of these six features. It is interesting to see that the time-of-flight variable keeps its importance almost unchanged for all momenta. Also, it looks like the \gls{ecal} energy ratio gets less relevant the higher the momentum is, but the RMS of the hit-to-cluster distance distribution and the maximum \gls{ecal} hit energy become more important in the high momentum ranges.

The last step in the feature selection analysis is the feature permutation. This technique measures the contribution of each feature to the performance of a model by randomly shuffling its values and checking how some scores degrade. For the present case, I am interested in the precision or purity, and the sensitivity or efficiency, as these two are the most relevant metrics from a physics point of view. The \texttt{scikit-learn} module provides the user with a method to perform the permutation scans.

The results of these are shown in Fig. \ref{fig:bdt_permutations}. For the different momentum ranges I show the permutation importances for the ten most important features. For each of the variables I report the effect the permutations have on the precision (blue) and sensitivity (yellow) of the models. The bars indicate the importance value, with the lighter part representing one standard deviation around the mean (hinted as an additional vertical line). Something to notice is that, in the first momentum region, the feature permutations have an effect on both the precision and the sensitivity. However, for the rest the precision is almost unaffected, while the sensitivity changes are considerably larger.

It is also interesting to see that most of the variables identified as important here are the same as I found when looking at the Shapley values. The behaviour of these across the momentum ranges is also similar, with the same patterns of some features being important at low momenta and then dropping in importance for the high momentum ranges.

With this, I conclude the study of the features. I have prepared the training and testing datasets and understood which features are likely to have the largest impact on the performance of the classifiers.

\subsection{Hyperparameter optimisation}

Any \gls{bdt} requires the user to specify a number of parameters that will dictate its behaviour. They can be divided into two categories: (i) tree-specific parameters, which affect each individual tree in the model, and (ii) boosting parameters, which control the boosting operation in the model. The value of these so-called hyperparameters affect the performance and predictive power of the models. Therefore, one needs to carefully select their optimal values in order to extract as much information as possible from the data.

From all the parameters used to define a tree in the \texttt{scikit-learn} implementation of the \gls{bdt} classifier, I only consider a subset of them. This is due to the fact that some are mutually exclusive, but also because I noticed that others have little effect on the problem at hand. Therefore, the parameters I investigate are the following:
\begin{itemize}
	\item \texttt{min_samples_split}: defines the minimum number of samples required in a node to be considered for splitting. High values prevent a model from learning relations which might be highly specific to the particular sample, but may led to underfitting.
	\item \texttt{min_samples_leaf}: defines the minimum samples required in a leaf node. For imbalanced problems it should take a low value, as there will not be many cases where the minority class dominates.
	\item \texttt{max_depth}: maximum depth of a tree. Useful to prevent overfitting, as high depths allow the model to learn relations specific to the training sample.
\end{itemize}
In the case of the boosting parameters, the ones I look at are:
\begin{itemize}
	\item \texttt{learning_rate}: determines the impact of each tree on the final outcome. Low values make the model robust to the specific characteristics of a tree, and thus allow it to generalise well. However, that usually requires a large number of trees to model the data properly.
	\item \texttt{n_estimators}: number of sequential trees to be trained. In general, \gls{bdt}s are fairly robust to high numbers of trees, but they can still overfit at a point.
	\item \texttt{subsample}: fraction of observations to be selected for each tree. Values slightly less than $1$ make the model robust by reducing the variance.
\end{itemize}

In general, hyperparameters depend on each other. Thus, it is not possible to optimise them independently. In the literature, we find two main strategies to explore the hyperparameter space. We could use a grid search, in which one discretises a portion of the space of hyperparameters and evaluates the model at each point. Another approach is the randomised search, where a certain number of random configurations of hyperparameters are explored.

In this case, I use the random search to scan the hyperparameter space. Also, because it is not guaranteed that a set of hyperparameters can be efficiently applied across different datasets, I perform the optimisation for each of the momentum ranges considered. Table \ref{tab:bdt_hyperpars} shows the list of hyperparameters considered, and the range within which I let them vary. I decided to fix the number of estimators to $400$ in all cases, as its value is correlated with that of the learning rate.

\begin{figure}[t]
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/precision_vs_sensitivity_p0_0.20_sigmap_0.10.pdf}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/precision_vs_sensitivity_p0_1.15_sigmap_0.35.pdf}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/BDT/precision_vs_sensitivity_p0_6.50_sigmap_3.50.pdf}
	\end{subfigure}
	\caption[Values of the precision and sensitivity obtained for $10^{4}$ \gls{bdt} hyperparameter configurations, for the momentum regions I, III and V.]{Values of the precision and sensitivity obtained for $10^{4}$ \gls{bdt} hyperparameter configurations, for the momentum regions I, III and V. The red contours indicate the curves of equal $F_{1}$-score, while the green crosses are the selected configurations.}
	\label{fig:hyper_precision_sensitivity}
\end{figure}

I evaluate $10^{4}$ different hyperparameter configurations for each momentum range. For the hyperparameter tuning, I use subsamples containing $10\%$ of the full datasets, keeping the original proportions between classes, in order to reduce the computational load. The performance of the models was assessed using a stratified 3-fold cross-validation with replacement. Cross-validation involves dividing the data in a number of subsets, training the model using some of them, and testing it with the rest. In our case, I divide the data in 3 equal-sized subsets, maintaining the class proportions of the original dataset. Then, for 3 consecutive iterations, I train the models using 2 of the subsets while I compute the precision and sensitivity scores with the other. This approach provides a more robust estimate of the performance on unseen data.

\begin{table}[t]
	\caption{Optimal values of the hyperparameters used by the \gls{bdt}, for each momentum range.}
	\begin{center}
		\begin{small}
			\begin{tabular}{l|l|lllll}
				\multirow{2}{*}{Hyperparameter} & \multirow{2}{*}{Range} & \multicolumn{5}{l}{Best value}                           \\[1mm] \cline{3-7}
												&                        & \rule{0pt}{1.1\normalbaselineskip}I & II & III & IV & V \\[1mm]
												\hline
				\rule{0pt}{1.1\normalbaselineskip}\texttt{min_samples_split}      & $[0.001, ~1]$          & 0.10     & 0.06      & 0.05       & 0.27      & 0.06     \\[2mm]
				\texttt{min_samples_leaf}       & $[0.001, ~1]$          & 0.02     & 0.03      & 0.01       & 0.02      & 0.07     \\[2mm]
				\texttt{max_depth}              & $\{2, ~3, ~..., ~8\}$  & 8        & 2         & 4          & 2         & 7      \\[2mm]
				\texttt{learning_rate}          & $[0.05, ~1]$           & 0.10     & 0.23      & 0.07       & 0.13      & 0.09     \\[2mm]
				\texttt{subsample}              & $[0.01, ~1]$           & 0.75     & 0.65      & 0.79       & 0.86      & 0.95    
			\end{tabular}
		\end{small}
	\end{center}
	\label{tab:bdt_hyperpars}
\end{table}

Figure \ref{fig:hyper_precision_sensitivity} shows the results in the precision versus sensitivity plane, for the momentum regions I, III and V (from left to right). The contours represent the curves of equal $F_{1}$-score, i.e. the harmonic mean of the precision and the sensitivity. The shape of the clusters in this space gives us an idea of the best possible performance of the \gls{bdt} when varying the hyperparameters. In order to select the optimal configurations (indicated in the plots with a green cross), I chose the point with the highest $F_{1}$-score.

The results for the different momentum ranges are summarised in Tab. \ref{tab:bdt_hyperpars}. One can see some consistency in hyperparameter choices, with models generally preferring small values for the tree-specific parameters, small learning rate, and relatively large subsample sizes.

\begin{table}[t]
	\caption{Performance metrics of the \gls{bdt}s with optimal hyperparameters, for the different momentum ranges.}
	\begin{center}
		\begin{small}
			\begin{tabular}{l|lllll}
				\multirow{2}{*}{Metric} & \multicolumn{5}{l}{Value $\pm ~ 1\sigma$}                                                                \\[2mm] \cline{2-6}
										& \rule{0pt}{1.1\normalbaselineskip}I                 & II                & III               & IV                & V                 \\[2mm] \hline
										\rule{0pt}{1.1\normalbaselineskip}Accuracy                & $0.779 \pm 0.003$ & $0.812 \pm 0.003$ & $0.846 \pm 0.002$ & $0.861 \pm 0.003$ & $0.874 \pm 0.002$ \\[2mm]
				Precision               & $0.769 \pm 0.003$ & $0.752 \pm 0.005$ & $0.788 \pm 0.002$ & $0.805 \pm 0.003$ & $0.815 \pm 0.003$ \\[2mm]
				Sensitivity             & $0.745 \pm 0.009$ & $0.921 \pm 0.006$ & $0.965 \pm 0.002$ & $0.967 \pm 0.002$ & $0.976 \pm 0.001$ \\[2mm]
				$F_{1}$-score           & $0.757 \pm 0.004$ & $0.828 \pm 0.003$ & $0.867 \pm 0.002$ & $0.879 \pm 0.002$ & $0.889 \pm 0.002$ \\[2mm]
				ROC AUC                 & $0.868 \pm 0.003$ & $0.865 \pm 0.003$ & $0.899 \pm 0.002$ & $0.902 \pm 0.002$ & $0.911 \pm 0.001$
			\end{tabular}
		\end{small}
	\end{center}
	\label{tab:bdt_metrics}
\end{table}

Now that I have obtained the optimal values of the hyperparameters, I can train the different \gls{bdt}s. In this case I use the complete datasets, keeping $20\%$ of the data for testing. Table \ref{tab:bdt_metrics} shows the values of the different performance metrics obtained using the selected hyperparameters and 5-fold cross-validation. The last row indicates the value of the area under the receiver operating characteristic (ROC) curve. This represents the sensitivity of a model as a function of the false positive rate. I have included it here as it is a classic model metric used in the machine learning community. Overall, there is a clear trend of models performing better at high momentum.

\subsection{Probability calibration}

So far, the trained \gls{bdt}s are able to provide predictions of the class labels. Ideally, one would like the output of a classifier to give a confidence level about the prediction. However, it is not straightforward to interpret the outputs of our \gls{bdt}s in terms of probabilities.

\begin{figure}[t]
	\centering
	\includegraphics[width=.75\linewidth]{Images/GArSoft_PID/BDT/ecal_bdt_output_example_calibration_curves.pdf}
	\caption[Reliability diagrams for the \gls{bdt} classifier used in the momentum range $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$.]{Reliability diagrams for the \gls{bdt} classifier used in the momentum range $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$, both for the original (blue circles) and calibrated (yellow squares) responses. For reference, the response of a perfectly calibrated classifier is also shown (black dashed line).}
	\label{fig:bdt_calibration_curves}
\end{figure}

A way to visualise how well the predictions of a classifier are calibrated is using reliability diagrams \cite{Wilks1995}. They represent the probability of the positive label versus the probability predicted by the classifier. These can be obtained by binning the predicted probabilities, and then compute the conditional probability $P(y_{true}=1|y_{i} \leq y_{pred} < y_{i+1})$ by checking the fraction of true positive instances in each bin. The reliability diagram of a perfectly calibrated classifier would be a diagonal line.

In this case, I try to correct the raw response of the classifiers by applying a sigmoid function:
\begin{equation}
	\sigma(x;~A,B) = \frac{1}{1+\mathrm{e}^{Ax+B}},
\end{equation}
where the parameters $A$ and $B$ are real numbers determined using the method of least squares.

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/ecal_bdt_output_example_calibration.pdf}
	\caption[Uncalibrated and calibrated predicted probabilities assigned by the \gls{bdt} classifier in the momentum range $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$.]{Uncalibrated (left panel) and calibrated (right panel) predicted probabilities assigned by the \gls{bdt} classifiers for true muons (blue) and charged pions (red) in the momentum range $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$.}
	\label{fig:bdt_calibration_output}
\end{figure}

For each classifier, I perform a grid search to obtain the optimal values of $A$ and $B$. For any pair, I compute the predicted probabilities as $y_{pred} = \sigma(y_{raw};~A,B)$, where $y_{raw}$ are the raw predictions of the classifier\footnote{In \texttt{scikit-learn} these correspond to the outputs of the \texttt{decision_function} method.}. Then, I calculate the corresponding reliability curve, and take the sum of the squared residuals between it and the response of the perfectly calibrated classifier.

Figure \ref{fig:bdt_calibration_curves} shows the reliability diagrams for the original (blue) and calibrated (yellow) probability predictions of the classifier for the III momentum range, $0.3 \leq p < 0.8 ~ \mathrm{GeV}/c$. The original response of the classifier is given by $y_{pred} = \sigma(y_{raw};~-2,0)$, which is the transformation applied by \texttt{scikit-learn} to produce the probability estimate. Notice how the calibrated prediction matches the ideal response much better than the original, across all the probability range.

One can also compare the responses of the uncalibrated and calibrated classifiers broken down by true particle type, as shown in Fig. \ref{fig:bdt_calibration_output}. It can be seen that the distributions for both muons (blue) and charged pions (red) smoothen after calibration, but still the separating power of the classifier remains unchanged.

\subsection{Performance}

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/BDT/ecal_bdt_validation_output_regions.pdf}
	\caption[Calibrated predicted probabilities assigned by the \gls{bdt} classifiers for the true muons and charged pions in a \gls{fhc} neutrino sample.]{Calibrated predicted probabilities assigned by the \gls{bdt} classifiers for the true muons (blue) and charged pions (red) in a \gls{fhc} neutrino sample.}
	\label{fig:bdt_validation_output}
\end{figure}

At this point, having the trained classifiers and the probability calibration parameters, I am able to assess the performance of the classification strategy in a physics-relevant case. For this, I prepared a sample of $10^{5}$ \gls{fhc} neutrino interaction events in the \gls{hpgtpc}. Using the truth matching information, I select all true muons and pions, and apply the corresponding \gls{bdt} classifier based on their momentum.

Figure \ref{fig:bdt_validation_output} shows the resulting calibrated output of the classifiers for the different momentum regions. I do not include the first region, $0.10 \leq p < 0.30 ~ \mathrm{GeV}/c$, as it only contains a small fraction of signal events. The distributions obtained for this validation sample appear quite consistent with the results found in the single particle case.

\begin{figure}[t]
	\centering
	\includegraphics[width=.75\linewidth]{Images/GArSoft_PID/BDT/ecal_bdt_validation_momentum.pdf}
	\caption[Efficiency and purity of the muon selection as a function of the reconstructed momentum for the \gls{fhc} neutrino sample.]{Efficiency (blue) and purity (red) of the muon selection as a function of the reconstructed momentum for the \gls{fhc} neutrino sample.}
	\label{fig:bdt_validation_output}
\end{figure}

I also studied the performance of the muon identification in a track-by-track selection. To do so, I apply a simple cut on the output of the \gls{bdt} classifiers. Every particle with a predicted probability higher than the cut is considered a muon, while the ones not passing the cut are taken to be pions. The results obtained for a cut of $0.50$ are shown in Fig. \ref{fig:bdt_validation_output}. Both the efficiency (blue) and the purity (red) of the selection are displayed as a function of the momentum. The binning was chosen so that there were no bins in between different momentum ranges and each had roughly the same number of events. Even without optimising the value of the cut, the performance of the selection is excellent. The only issues appear in the first momentum range, where efficiency and purity sit slightly below $0.50$. However, a $\mathrm{d}E/\mathrm{d}x$ measurement could help enhance the selection there.

This shows that the method behaves as expected when using unseen simulated data, generalising without problems from single particle to full neutrino events. In the coming Chapter, I will study how to use the outputs of the \gls{bdt}, hereafter referred to as muon scores, to perform realistic event selections in \gls{ndgar}.

\section{ECal time-of-flight}\label{section:tof}

Looking at Fig. \ref{fig:dEdx_vs_momentum_regions}, it is clear that for momentum values in the range $1.0-3.0 ~ \mathrm{GeV}/c$ it is not possible to separate pions and protons using a $\left<\mathrm{d}E/\mathrm{d}x\right>$ measurement in the \gls{hpgtpc}. However, in the previous section I assumed that protons at those energies could be identified by other means, and therefore were not an issue for the muon and pion discrimination.

Some detectors, like \gls{alice} \cite{ALICE2011} or the ILD concept \cite{Einhaus2021}, complement the \gls{pid} capabilities of their gaseous trackers with time-of-flight (\gls{tof}) measurements. The use of fast timing silicon sensors, with hit time resolutions under $100~\mathrm{ps}$, would allow for the identification of charged hadrons via a \gls{tof} measurement up to $5.0 ~ \mathrm{GeV}/c$. In the case of \gls{ndgar}, one could think of using the inner layers of the \gls{ecal}, the ones consisting of high-granularity tiles, to obtain a \gls{tof}-based \gls{pid}, with some inputs from the \gls{tpc}.

Measuring the momentum and the velocity of a charged particle allows for a determination of the mass through the relativistic momentum formula:
\begin{equation}\label{8.19}
	m = \frac{p}{\beta} \sqrt{1-\beta^{2}}.
\end{equation}
In our case, the momentum is measured in the \gls{tpc}, using the curvature and the dip angle of the helix inside the magnetic field. The velocity of the particle can be written as:
\begin{equation}
	\beta = \frac{\ell_{track}}{c \tau},
\end{equation}
where $\ell_{track}$ is the length of the track, and $\tau$ the arrival time to the \gls{ecal}.

In GArSoft, the track length is computed at the Kalman filter stage. It is simply the sum of the line segments along the track, either in the forward or backward fit. In this case, because we are only interested in the particles that make it to the \gls{ecal}, I choose the fit direction based on the results of the track-cluster associations.

\begin{figure}[t]
	\centering
	\includegraphics[width=.75\linewidth]{Images/GArSoft_PID/tof/tof_diagram.png}
	\caption[Schematic diagram of the hit selection used for the \gls{ecal} \gls{tof} measurement.]{Schematic of the hit selection used for the \gls{tof} measurement. The grid represents the layers of the inner \gls{ecal}, with coloured squares indicating the tiles with hits. Green squares indicate the selected hits.}
	\label{fig:tof_diagram}
\end{figure}

Additionally, because the last $25~\mathrm{cm}$ of the \gls{hpgtpc} radius is uninstrumented, I need to correct for the length of the tracks. Using the track fit parameters to propagate the helix to its entry point in the \gls{ecal}, one can write the total track length as:
\begin{equation}
	\ell_{track} = \ell + \left| \frac{\phi_{\mathrm{EP}} - \phi}{R^{-1}} \right| \sqrt{1+\mathrm{tan}^{2}\lambda},
\end{equation}
where $\phi_{\mathrm{EP}}$ is the angle of rotation at the entry point to the calorimeter, and $\ell$, $\phi$, $R$ and $\lambda$ are the track length, angle of rotation, radius of curvature and dip angle at the last point in the fit, respectively.

To test the idea of performing a \gls{tof} measurement with the inner \gls{ecal}, I generated two data samples. Each consists of $10^{4}$ single particle events, either charged pions or protons. Their momenta are uniformly distributed in the range $0.5-5.0~\mathrm{GeV}/c$, and their directions are isotropic. I process each sample using different values of the time resolution, from $\Delta \tau = 0$, the perfect time resolution case for comparison, to the current nominal value of $\Delta \tau = 0.7~\mathrm{ns}$, and the worse scenario of $\Delta \tau = 1.0~\mathrm{ns}$.

\subsection{Arrival time estimations}

In the simulation, the limited time resolution of the \gls{ecal} is taken into account by applying a Gaussian smearing to the true hit times. Other effects, like the digitisation of the signals, are not taken into account and fall beyond the scope of this study. After the track-cluster association, one ends up with a collection of \gls{ecal} hits associated to each particle. From these, the arrival time of the particle to the \gls{ecal} can be extracted.

The simplest possibilities are to either take the time of the earliest hit or the hit closest to the entry point. Because in general these two coincide, I focus only on the earliest hit time. However, this needs to be corrected, to account for the distance travelled from the entry point to the position of the hit:
\begin{equation}\label{8.22}
	\tau_{earliest} = \tau_{hit} - \frac{d_{\mathrm{EP}-hit}}{c},
\end{equation}
where $\tau_{hit}$ is the time of the earliest hit, and $d_{\mathrm{EP}-hit}$ is the distance between that hit and the entry point of the particle to the \gls{ecal}. This is computed as the arc length between the entry point and the point of the extrapolated helix up to the layer of the hit. This way of correcting the time assumes $c$ for the velocity of the particle, which may lead to biased estimates.

\begin{figure}[t]
	\centering
	\includegraphics[width=.99\linewidth]{Images/GArSoft_PID/tof/beta_vs_momentum_comparison_delta_0.1.pdf}
	\caption[Particle velocity versus momentum measured with different \gls{ecal} arrival time estimations.]{Particle velocity versus momentum measured with different \gls{ecal} arrival time estimations. From left to right: earliest hit time, average hit time, and fitted hit time. In all cases the time resolution is $\Delta \tau = 0.1 ~ \mathrm{ns}$.}
	\label{fig:tof_beta_comp}
\end{figure}

I also tried to estimate the arrival times using information from the rest of the hits. In order to do this, as a simplifying assumption, I approximate the hadronic shower considering only its MIP component. For each layer, I keep only the hit in the tile closest to the point of the extrapolated track up to that layer. Figure \ref{fig:tof_diagram} shows an example of how this hit selection works. The dashed line represents the extrapolated track, while the coloured squares are the tiles containing hits. I use green to indicate the tile closest to the track in each layer (in the sketch they correspond to the grid columns).

Now, I can use these collections of hits to estimate the arrival times. A possibility is to take the average of the times of the selected hits, denoted $\tau_{average}$. For that to work, one needs to correct these times, in a similar way as in Eq. (\ref{8.22}), before taking the average. However, as before, this correction assumes that the particle travels at the speed of light inside the \gls{ecal}. Another option is to perform a linear fit to the hit times and the distances to the entry point. In that case, the arrival time would be the fitted value of the intercept, $\tau_{fit}$. This method would not assume a speed of light propagation.

Figure \ref{fig:tof_beta_comp} shows the velocity estimations as a function of the particle momentum, for the earliest hit time (left panel), average hit time (middle panel), and fitted hit time (right panel), for a time resolution of $\Delta \tau = 0.1 ~ \mathrm{ns}$. The two bands correspond to the $\pi^{\pm}$ and the $p$ particles. Notice how, for the earliest hit time method, the velocities are significantly biased towards larger values. For the multi-hit methods, the $\tau_{fit}$ estimate appears to produce a larger variance than when using the $\tau_{average}$ method.

\subsection{Proton and pion separation}

Once we have the velocities of the particles, one can estimate their masses through Eq. (\ref{8.19}). The resulting mass spectra are shown in Fig. \ref{fig:tof_mass_spectra}. I computed the masses for the three arrival time estimates discussed above, and three different values of the time resolution: $\Delta \tau = 0.00$ (perfect time resolution), $\Delta \tau = 0.10 ~ \mathrm{ns}$, and $\Delta \tau = 0.70 ~ \mathrm{ns}$. Although in all cases we have the same number of events, it appears as if the entries in the histograms decrease as the time resolution increases. Sometimes, the particles get unphysical values of $\beta > 1$, and in turn they do not contribute to the mass spectra. This is more likely to happen for higher values of $\Delta \tau$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/tof/reco_mass_comparison.pdf}
	\caption[Mass spectra for $p$ and $\pi^{\pm}$ particles, using different \gls{ecal} time resolution values and arrival time estimates.]{Mass spectra for $p$ (blue) and $\pi^{\pm}$ (yellow) particles, using different \gls{ecal} time resolution values (from top to bottom, in ascending order), and arrival time estimates. From left to right: earliest hit time, average hit time, and fitted hit time. The dashed lines indicate the true masses of the particles.}
	\label{fig:tof_mass_spectra}
\end{figure}

As noted before, the average hit time method produces the most robust estimates when increasing $\Delta \tau$. Intuitively this makes sense, as by taking the mean one averages out the effect of the Gaussian smearing. Going forward, I will use this arrival time estimator, as it appears to be the best performing one.

\begin{figure}[t]
	\centering
	\includegraphics[width=.75\linewidth]{Images/GArSoft_PID/tof/proton_selection_beta_metrics.pdf}
	\caption[Efficiency and purity for the proton selection as a function of the momentum, for $\Delta \tau = 0.10 ~ \mathrm{ns}$.]{Efficiency (top panel) and purity (bottom panel) for the proton selection as a function of the momentum, for $\Delta \tau = 0.10 ~ \mathrm{ns}$.}
	\label{fig:tof_beta_selection}
\end{figure}

It is possible to use the velocity estimations to select a sample of protons. In this case, I do so by dividing the relevant momentum range in bins of $0.1~\mathrm{GeV}/c$. For each momentum bin, I compute the expected velocity for the protons via the inverse of Eq. (\ref{8.19}), and then take the fractional residuals of the measured velocities. Using that distribution, I choose the cut that maximises the $F_{1}$-score of the proton selection.

The results can be seen in Fig. \ref{fig:tof_beta_selection}, for the case $\Delta \tau = 0.10 ~ \mathrm{ns}$. As expected from Fig. \ref{fig:tof_beta_comp}, the performance of the selection degrades rapidly with increasing momentum. However, the purity is still around $75\%$ at $3.0~\mathrm{GeV}/c$. This is likely to be sufficient, as we do not expect protons or charged pions with higher energies from the beam neutrino interactions.

\begin{figure}[t]
	\centering
	\includegraphics[width=.95\linewidth]{Images/GArSoft_PID/tof/numu_cc_proton_tof_summary.pdf}
	\caption{Distributions of the velocities measured by \gls{tof} with the
	inner \gls{ecal}, for different momentum bins, in a \gls{fhc} neutrino interaction sample. The Gaussian fits are performed around the maxima for each particle species.}
	\label{fig:tof_beta_fhc}
\end{figure}

Figure \ref{fig:tof_beta_fhc} shows a few examples of the \gls{tof} velocity estimation in a \gls{fhc} neutrino sample. Here, for the different momentum bins, I have taken the fractional residual of the expected value of $\beta$ for a proton and the measured values (black data points). The coloured lines represent Gaussian fits to the distributions of the different true particle, with the gray line being the sum of these. It can be seen that, even for momenta close to $2.0~\mathrm{GeV}/c$, a good proton separation can be achieved. This idea will be explored further later, in the context of the event selection.

\section{Integration in GArSoft}\label{section:integration}

All the additions and improvements to the reconstruction discussed in this Chapter had to be integrated in the GArSoft framework. This is necessary both to allow a more streamlined path for development, as this makes testing and adding features straightforward, as well as make the changes usable in future productions of simulated data. In this section, I outline the current status of the integration in GArSoft of the reconstruction work presented above.

The new track-cluster association code has been implemented in GArSoft, under the name of \texttt{TPCECALAssociation2}, and has now become the new default in the reconstruction. The structure of the module is similar to the previous implementation, and the data products they output are identical in form. Therefore, any existing code using the association objects does not need to be modified.

\begin{figure}[t]
    \centering
    \includegraphics[width=.99\linewidth]{Images/GArSoft_PID/caf_proton_scores.pdf}
    \caption[Distributions of proton $\mathrm{d}E/\mathrm{d}x$ and \gls{tof} scores for a sample of 100000 \gls{fhc} neutrino interactions in the \gls{hpgtpc}.]{Distributions of proton $\mathrm{d}E/\mathrm{d}x$ (left panel) and \gls{tof} (right panel) scores for a sample of 100000 \gls{fhc} neutrino interactions in the \gls{hpgtpc}. The distributions are broken down by the true particle type associated to the reconstructed particle.}
    \label{fig:proton_scores}
\end{figure}

The computation of the truncated mean $\mathrm{d}E/\mathrm{d}x$ of the tracks, the evaluation of the muon score for muon and pion separation, and the estimation of the velocity from time-of-flight are all orchestrated by the new \texttt{CreateRecoParticles} module. Each one of these is implemented as a separate algorithm, which is then called by the parent module. It generates the \texttt{gar::rec::RecoParticle} products, a new high-level data object in GArSoft. These combine the information from the \gls{hpgtpc}, \gls{ecal}, and \gls{muid} to create an object useful for analysers. At the moment, these data products are only generated for charged particles. However, in the future the module can be extended to incorporate other algorithms used for the identification of neutral particles, like neutral pions and neutrons.

Additionally, analogous to the muon score, the \texttt{gar::rec::RecoParticle} objects contain two other scores based on the $\left<\mathrm{d}E/\mathrm{d}x\right>$ and \gls{tof} estimates which measure the ``protoness'' of a reconstructed particles. These are obtained in a number of momentum bins, and are a measure of the distance to the point in the corresponding distribution that maximises the $F_{1}$-score for the proton separation. This distance is then transformed applying a sigmoid function, which produces a score in the $0-1$ range, with coefficients obtained following a procedure similar to the one used to calibrate the response of the muon score. The $\mathrm{d}E/\mathrm{d}x$ proton score is defined for all particles with momenta $p_{\mathrm{reco}} < 1.5~\mathrm{GeV}/c$, whereas the \gls{tof} proton score is available for the particles with at least one associated hit in the inner \gls{ecal} and momentum in the range $0.5 \leq p_{\mathrm{reco}} < 3.0~\mathrm{GeV}/c$. As an example, Fig. \ref{fig:proton_scores} shows the distributions of the $\mathrm{d}E/\mathrm{d}x$ (left panel) and \gls{tof} (right panel) proton scores for the reconstructed particles in a $10^{5}$ \gls{fhc} neutrino sample.

The calculation of the track breakpoint variables for pion decay identification discussed in App. \ref{section:pi_decay} is currently implemented as an analysis module in GArSoft. It would be interesting to add this information to the \texttt{gar::rec::RecoParticle} products, possibly calling the code as an additional algorithm in the \texttt{CreateRecoParticles} module. However, the best way to propagate the information to the high-level objects is still unclear.

The new \gls{ecal} clustering algorithm proposed in App. \ref{section:neutral} is still in a development phase, and as such it has not replaced the current clustering module. At the moment, its latest version is integrated in GArSoft as the \texttt{CaloClustering2} module. The algorithm used is implemented separately, and then invoked in the main code. The module can be run standalone on the outputs of the reconstruction, creating a second instance of the \texttt{gar::rec::Cluster} collection. In the future it may replace the current algorithm as the default in the reconstruction chain. However, more work is needed in order to understand its performance in all the different use cases.

\section{Summary}

This Chapter reviews my work on the reconstruction for \gls{ndgar}. In section \ref{section:dEdx} I describe how to use the calibrated energy deposits in the \gls{hpgtpc} to compute the mean $\mathrm{d}E/\mathrm{d}x$ for the reconstructed particles. I finish the section providing a parametrisation for how this depends on the momentum. The problem of the muon and pion separation is the topic of section \ref{section:muon_bdt}. I propose to use the information from the \gls{ecal} to achieve this classification. In this section, I describe the features and the procedure I follow to train the classifier, showing its performance as a function of the particle momentum. In section \ref{section:tof} I explore the possibility of performing a \gls{tof} measurement with the \gls{ecal}. With this, I achieve a separation between pions and protons in a momentum range beyond the reach of the \gls{hpgtpc} alone. Finally, in section \ref{section:integration} I describe the status of the integration of the different additions to the reconstruction chain.

The contributions to the reconstruction described in this Chapter represent the first attempt at a realistic \gls{pid} using the end-to-end \gls{ndgar} simulation. These \gls{pid} metrics can already be used to explore the event selection capabilities of the detector, which is the topic of the next Chapter. The parallel development of the reconstruction and the selection strategies will allow for a physics-driven optmisation of our algorithms, based on the specific needs of the experiment.